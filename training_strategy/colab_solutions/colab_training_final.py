# üöÄ FINAL TRAINING CODE - MAMUT R600 Turkish Financial Q&A
# Run this AFTER comprehensive fix + restart + verification

print("üöÄ MAMUT R600 - FINAL TURKISH AI TRAINING")
print("=" * 60)

# Initial imports and checks
import torch
import numpy as np
print(f"üî• GPU: {torch.cuda.is_available()}")
print(f"üìä Numpy: {np.__version__}")

if torch.cuda.is_available():
    print(f"üíæ GPU: {torch.cuda.get_device_name(0)}")
    print(f"üîã Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

# Package version check
try:
    import transformers
    import datasets 
    import accelerate
    from huggingface_hub import login
    print(f"‚úÖ Transformers: {transformers.__version__}")
    print(f"‚úÖ Datasets: {datasets.__version__}")
    print(f"‚úÖ Accelerate: {accelerate.__version__}")
except ImportError as e:
    print(f"‚ùå Import error: {e}")
    print("üîÑ Please run comprehensive fix again!")
    exit()

# HuggingFace Authentication
HF_TOKEN = "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"
HF_MODEL_NAME = "rsmctn/turkish-financial-qa-v1"

try:
    login(HF_TOKEN)
    print("‚úÖ HuggingFace authenticated!")
except Exception as e:
    print(f"‚ùå HF Auth error: {e}")

# Training Data - Turkish Financial Q&A
training_data = [
    {
        "question": "GARAN hissesi bug√ºn nasƒ±l performans g√∂steriyor?",
        "context": "T√ºrkiye Garanti Bankasƒ± A.≈û. (GARAN) hissesi bug√ºn ‚Ç∫89.30 fiyatƒ±nda, g√ºnl√ºk %-0.94 deƒüi≈üimle i≈ülem g√∂rmektedir. Bankacƒ±lƒ±k sekt√∂r√ºnde yer alan hisse, son 52 haftada ‚Ç∫65.20 - ‚Ç∫95.40 bandƒ±nda hareket etmi≈ütir.",
        "answer": "GARAN hissesi %-0.94 d√º≈ü√º≈ü g√∂stererek ‚Ç∫89.30'da i≈ülem g√∂rmektedir"
    },
    {
        "question": "RSI g√∂stergesi nedir?",
        "context": "RSI (Relative Strength Index) 0-100 arasƒ±nda deƒüer alan bir momentum osilat√∂r√ºd√ºr. 70 √ºzerindeki deƒüerler a≈üƒ±rƒ± alƒ±m b√∂lgesini, 30 altƒ±ndaki deƒüerler a≈üƒ±rƒ± satƒ±m b√∂lgesini g√∂sterir.",
        "answer": "RSI, 0-100 arasƒ±nda deƒüer alan momentum g√∂stergesidir"
    },
    {
        "question": "BIST 100 endeksi bug√ºn nasƒ±l?",
        "context": "BIST 100 endeksi bug√ºn 8,450.75 seviyesinde, g√ºnl√ºk %1.25 artƒ±≈üla kapanmƒ±≈ütƒ±r. ƒ∞≈ülem hacmi 18.5 milyar TL olarak ger√ßekle≈ümi≈ütir.",
        "answer": "BIST 100 endeksi %1.25 y√ºkseli≈üle 8,450.75 seviyesinde kapanmƒ±≈ütƒ±r"
    },
    {
        "question": "Teknik analiz nedir?",
        "context": "Teknik analiz, ge√ßmi≈ü fiyat hareketleri ve i≈ülem hacmi verilerini kullanarak gelecekteki fiyat hareketlerini tahmin etmeye √ßalƒ±≈üan analiz y√∂ntemidir. RSI, MACD, Bollinger Bantlarƒ± gibi g√∂stergeler kullanƒ±r.",
        "answer": "Teknik analiz, fiyat verilerini kullanarak gelecek tahminleri yapan y√∂ntemdir"
    },
    {
        "question": "AKBNK i√ßin stop loss nerede?",
        "context": "AKBNK hissesi ‚Ç∫69.00 seviyesinde i≈ülem g√∂rmektedir. √ñnemli destek seviyesi ‚Ç∫65.20 civarƒ±ndadƒ±r. Volatilite %2.5 seviyesinde.",
        "answer": "AKBNK i√ßin stop loss ‚Ç∫65.00-‚Ç∫66.50 aralƒ±ƒüƒ±nda belirlenebilir"
    },
    {
        "question": "Piyasa durumu nasƒ±l?",
        "context": "BIST 100 endeksi %1.25 y√ºkseli≈üte, yabancƒ± yatƒ±rƒ±mcƒ±lar net 125 milyon TL alƒ±mda bulundu. Bankacƒ±lƒ±k endeksi %2.1 artƒ±≈ü g√∂sterdi.",
        "answer": "Piyasa pozitif seyrediyor, yabancƒ± net alƒ±mda"
    },
    {
        "question": "MACD nasƒ±l yorumlanƒ±r?",
        "context": "MACD (Moving Average Convergence Divergence) trend takip g√∂stergesidir. MACD √ßizgisinin sinyal √ßizgisini yukarƒ± kesmesi alƒ±m sinyali verir.",
        "answer": "MACD > Sinyal √ßizgisi alƒ±m sinyali verir"
    },
    {
        "question": "Risk y√∂netimi nasƒ±l yapƒ±lƒ±r?",
        "context": "Risk y√∂netimi portf√∂y √ße≈üitlendirmesi, stop-loss kullanƒ±mƒ± i√ßerir. Toplam portf√∂y√ºn %2'sinden fazlasƒ± tek i≈ülemde riske edilmemelidir.",
        "answer": "Portf√∂y√º √ße≈üitlendirin, stop-loss kullanƒ±n"
    }
]

print(f"‚úÖ {len(training_data)} Turkish Q&A samples loaded")

# Load Model with error handling
try:
    from transformers import AutoTokenizer, AutoModelForQuestionAnswering
    
    model_name = "dbmdz/bert-base-turkish-cased"
    print(f"üì• Loading {model_name}...")
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForQuestionAnswering.from_pretrained(model_name)
    
    print(f"‚úÖ Model loaded: {model.num_parameters():,} parameters")
    
except Exception as e:
    print(f"‚ùå Model loading error: {e}")
    print("üîÑ Trying alternative approach...")
    # Could add fallback model here

# Data preprocessing with robust error handling
from datasets import Dataset

def robust_preprocess(examples):
    """Robust preprocessing with fallback options"""
    try:
        questions = examples["question"] if isinstance(examples["question"], list) else [examples["question"]]
        contexts = examples["context"] if isinstance(examples["context"], list) else [examples["context"]]
        answers = examples["answer"] if isinstance(examples["answer"], list) else [examples["answer"]]
        
        # Tokenize with error handling
        inputs = tokenizer(
            questions,
            contexts,
            max_length=384,
            truncation=True,
            padding="max_length",
            return_tensors="pt"
        )
        
        # Simple answer positioning
        start_positions = []
        end_positions = []
        
        for i in range(len(questions)):
            try:
                context = contexts[i]
                answer = answers[i]
                
                # Find answer in context
                start_char = context.find(answer)
                if start_char >= 0:
                    # Simple token position estimation
                    tokens_before = len(tokenizer.tokenize(context[:start_char]))
                    answer_tokens = len(tokenizer.tokenize(answer))
                    
                    start_pos = min(tokens_before + 1, 380)  # +1 for [CLS]
                    end_pos = min(start_pos + answer_tokens - 1, 383)
                else:
                    # Fallback positions
                    start_pos = 1
                    end_pos = min(len(tokenizer.tokenize(answer)) + 1, 10)
                    
                start_positions.append(start_pos)
                end_positions.append(end_pos)
                
            except Exception as e:
                print(f"Warning: Error processing sample {i}: {e}")
                start_positions.append(1)
                end_positions.append(2)
        
        inputs["start_positions"] = torch.tensor(start_positions, dtype=torch.long)
        inputs["end_positions"] = torch.tensor(end_positions, dtype=torch.long)
        
        return inputs
        
    except Exception as e:
        print(f"‚ùå Preprocessing error: {e}")
        return None

# Create dataset
try:
    dataset_dict = {
        "question": [item["question"] for item in training_data],
        "context": [item["context"] for item in training_data],
        "answer": [item["answer"] for item in training_data]
    }
    
    # Process each example individually for robustness
    processed_examples = []
    for i, item in enumerate(training_data):
        try:
            single_example = {
                "question": [item["question"]],
                "context": [item["context"]],
                "answer": [item["answer"]]
            }
            
            processed = robust_preprocess(single_example)
            if processed is not None:
                processed_examples.append({
                    "input_ids": processed["input_ids"][0],
                    "attention_mask": processed["attention_mask"][0],
                    "start_positions": processed["start_positions"][0],
                    "end_positions": processed["end_positions"][0]
                })
            else:
                print(f"‚ö†Ô∏è Skipping sample {i} due to preprocessing error")
                
        except Exception as e:
            print(f"‚ö†Ô∏è Error processing sample {i}: {e}")
            continue
    
    final_dataset = Dataset.from_list(processed_examples)
    print(f"‚úÖ Dataset created: {len(final_dataset)} valid samples")
    
except Exception as e:
    print(f"‚ùå Dataset creation error: {e}")

# Training setup with conservative settings
try:
    from transformers import TrainingArguments, Trainer, DefaultDataCollator
    from datetime import datetime
    
    training_args = TrainingArguments(
        output_dir="./turkish-qa-model",
        learning_rate=2e-5,  # Conservative learning rate
        num_train_epochs=3,   # Reduced epochs
        per_device_train_batch_size=1,  # Small batch size for stability
        per_device_eval_batch_size=1,
        gradient_accumulation_steps=4,  # Compensate for small batch
        weight_decay=0.01,
        warmup_steps=5,
        evaluation_strategy="steps",
        eval_steps=10,
        save_steps=10,
        save_total_limit=2,
        load_best_model_at_end=True,
        logging_steps=2,
        push_to_hub=True,
        hub_model_id=HF_MODEL_NAME,
        hub_strategy="end",
        fp16=torch.cuda.is_available(),
        dataloader_pin_memory=False,
        remove_unused_columns=False,
    )
    
    print("‚úÖ Training configuration ready (conservative settings)")
    
except Exception as e:
    print(f"‚ùå Training setup error: {e}")

# Create Trainer and start training
try:
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=final_dataset,
        eval_dataset=final_dataset,
        tokenizer=tokenizer,
        data_collator=DefaultDataCollator(),
    )
    
    print("üî• STARTING TRAINING...")
    print("=" * 50)
    print(f"‚è∞ Start: {datetime.now().strftime('%H:%M:%S')}")
    
    # Training with error handling
    try:
        train_result = trainer.train()
        print("üéâ TRAINING COMPLETED SUCCESSFULLY!")
        print(f"üìä Final Loss: {train_result.training_loss:.4f}")
        
    except RuntimeError as e:
        if "out of memory" in str(e).lower():
            print("‚ö†Ô∏è GPU Memory error, trying smaller batch...")
            # Reduce batch size even more
            training_args.per_device_train_batch_size = 1
            training_args.gradient_accumulation_steps = 8
            training_args.fp16 = True
            
            trainer = Trainer(
                model=model,
                args=training_args,
                train_dataset=final_dataset,
                eval_dataset=final_dataset,
                tokenizer=tokenizer,
                data_collator=DefaultDataCollator(),
            )
            
            train_result = trainer.train()
            print("‚úÖ Training completed with reduced batch size!")
        else:
            raise e
            
    print(f"‚è∞ End: {datetime.now().strftime('%H:%M:%S')}")
    
except Exception as e:
    print(f"‚ùå Training error: {e}")

# Model testing
try:
    print("\nüß™ TESTING MODEL...")
    print("=" * 30)
    
    from transformers import pipeline
    
    qa_pipeline = pipeline(
        "question-answering",
        model=trainer.model,
        tokenizer=tokenizer,
        device=0 if torch.cuda.is_available() else -1
    )
    
    test_cases = [
        ("GARAN nasƒ±l?", "GARAN hissesi ‚Ç∫89.30'da %-0.94 d√º≈ü√º≈üte."),
        ("RSI nedir?", "RSI momentum g√∂stergesidir. 70 √ºst√º a≈üƒ±rƒ± alƒ±m."),
        ("Stop loss nerede?", "Stop loss destek altƒ±nda belirlenmelidir.")
    ]
    
    for i, (q, c) in enumerate(test_cases, 1):
        try:
            result = qa_pipeline(question=q, context=c)
            print(f"Test {i}: {q}")
            print(f"‚úÖ AI: {result['answer']}")
            print(f"üéØ Confidence: {result['score']:.3f}")
            print("-" * 25)
        except Exception as e:
            print(f"Test {i} error: {e}")
            
except Exception as e:
    print(f"‚ùå Testing error: {e}")

# Upload to HuggingFace
try:
    print("üöÄ UPLOADING TO HUGGINGFACE...")
    trainer.push_to_hub(commit_message="Turkish Financial Q&A - MAMUT R600")
    print("üéâ MODEL UPLOADED SUCCESSFULLY!")
    print(f"üìç URL: https://huggingface.co/{HF_MODEL_NAME}")
    
except Exception as e:
    print(f"‚ö†Ô∏è Upload error: {e}")
    print("üíæ Saving locally instead...")
    try:
        model.save_pretrained("./turkish-qa-model")
        tokenizer.save_pretrained("./turkish-qa-model")
        print("‚úÖ Model saved locally!")
    except Exception as save_e:
        print(f"‚ùå Local save error: {save_e}")

# Final summary
print("\n" + "="*60)
print("üéâ TRAINING PROCESS COMPLETED!")
print("="*60)
print(f"‚úÖ Model: {HF_MODEL_NAME}")
print(f"‚úÖ Samples trained: {len(final_dataset)}")
print("‚úÖ Turkish Financial Q&A AI ready!")

if "üéâ MODEL UPLOADED SUCCESSFULLY!" in locals():
    print("‚úÖ HuggingFace upload: SUCCESS")
    print(f"üîó API: https://api-inference.huggingface.co/models/{HF_MODEL_NAME}")
else:
    print("‚ö†Ô∏è HuggingFace upload: Failed (but model trained)")

print("=" * 60)
print("üöÄ NEXT: Integrate with Railway API!")
print("üìã Railway integration code will be provided separately")
print("=" * 60)
