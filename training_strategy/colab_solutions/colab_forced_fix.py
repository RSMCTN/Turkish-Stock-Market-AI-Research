# ğŸš€ FORCED VERSION FIX - KÃ¶klÃ¼ Ã§Ã¶zÃ¼m!
# Known working versions ile force install

print("ğŸš€ FORCED VERSION FIX - MAMUT R600")
print("=" * 60)

print("ğŸ¯ STRATEGY: Force install known working version combination")
print("Based on: transformers 4.35.0 + huggingface_hub 0.17.3 + accelerate 0.24.0")
print()

# STEP 1: Check current state
print("1ï¸âƒ£ CURRENT STATE CHECK")
print("=" * 30)

import sys
print(f"Python: {sys.version}")

try:
    import torch
    print(f"PyTorch: {torch.__version__}")
except:
    print("PyTorch: Not available")

# STEP 2: Force install working combination
print("\n2ï¸âƒ£ FORCING WORKING VERSION COMBINATION")
print("=" * 30)

print("ğŸ”§ Uninstalling conflicting packages...")
!pip uninstall -y transformers huggingface_hub accelerate datasets peft sentence-transformers -q

print("ğŸ”§ Installing known working versions in correct order...")

# Install in dependency order to avoid conflicts
print("  Installing huggingface_hub 0.17.3...")
!pip install huggingface_hub==0.17.3 --no-deps -q

print("  Installing accelerate 0.24.0...")  
!pip install accelerate==0.24.0 --no-deps -q

print("  Installing datasets 2.14.0...")
!pip install datasets==2.14.0 --no-deps -q

print("  Installing transformers 4.35.0...")
!pip install transformers==4.35.0 --no-deps -q

print("âœ… Forced installation complete!")

# STEP 3: Install missing dependencies manually
print("\n3ï¸âƒ£ INSTALLING MISSING DEPENDENCIES")
print("=" * 30)

# Transformers dependencies
!pip install tokenizers>=0.13.3 -q
!pip install safetensors>=0.3.1 -q  
!pip install regex -q
!pip install requests -q
!pip install tqdm -q

# Datasets dependencies  
!pip install pyarrow -q
!pip install dill -q
!pip install pandas -q
!pip install fsspec -q

# Accelerate dependencies
!pip install psutil -q
!pip install packaging -q

print("âœ… Dependencies installed!")

# STEP 4: Critical validation test
print("\n4ï¸âƒ£ CRITICAL VALIDATION TEST")
print("=" * 30)

validation_passed = True

try:
    print("Testing huggingface_hub import...")
    import huggingface_hub
    print(f"âœ… huggingface_hub: {huggingface_hub.__version__}")
    
    # Check for the problematic function
    if hasattr(huggingface_hub, 'list_repo_tree'):
        print("âœ… list_repo_tree: Available")
    else:
        print("âš ï¸ list_repo_tree: Not available (but may not be needed)")
        
except Exception as e:
    print(f"âŒ huggingface_hub error: {e}")
    validation_passed = False

try:
    print("Testing transformers import...")
    import transformers
    print(f"âœ… transformers: {transformers.__version__}")
except Exception as e:
    print(f"âŒ transformers error: {e}")
    validation_passed = False

try:
    print("Testing accelerate import...")
    import accelerate  
    print(f"âœ… accelerate: {accelerate.__version__}")
except Exception as e:
    print(f"âŒ accelerate error: {e}")
    validation_passed = False
    
try:
    print("Testing datasets import...")
    import datasets
    print(f"âœ… datasets: {datasets.__version__}")  
except Exception as e:
    print(f"âŒ datasets error: {e}")
    validation_passed = False

# STEP 5: Model loading test
if validation_passed:
    print("\n5ï¸âƒ£ MODEL LOADING TEST")
    print("=" * 30)
    
    try:
        from transformers import AutoTokenizer, AutoModelForQuestionAnswering
        
        print("Loading Turkish BERT model...")
        model_name = "dbmdz/bert-base-turkish-cased"
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForQuestionAnswering.from_pretrained(model_name)
        
        print("âœ… Turkish BERT model loaded successfully!")
        
        # Quick functionality test
        print("Testing basic tokenization...")
        test_text = "GARAN hissesi bugÃ¼n nasÄ±l?"
        tokens = tokenizer.encode(test_text)
        print(f"âœ… Tokenization working: {len(tokens)} tokens")
        
    except Exception as e:
        print(f"âŒ Model loading error: {e}")
        validation_passed = False

# STEP 6: If validation passed, do minimal training
if validation_passed:
    print("\n6ï¸âƒ£ MINIMAL TRAINING TEST")
    print("=" * 30)
    
    # HuggingFace auth
    from huggingface_hub import login
    
    HF_TOKEN = "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"
    HF_MODEL_NAME = "rsmctn/turkish-financial-qa-fixed"
    
    try:
        login(HF_TOKEN)
        print("âœ… HuggingFace authenticated!")
    except Exception as e:
        print(f"âš ï¸ Auth warning: {e}")
    
    # Minimal training data
    training_data = [
        {
            "question": "GARAN hissesi nasÄ±l?",
            "context": "GARAN hissesi â‚º89.30'da %-0.94 dÃ¼ÅŸÃ¼ÅŸle iÅŸlem gÃ¶rÃ¼yor.",
            "answer": "%-0.94 dÃ¼ÅŸÃ¼ÅŸle iÅŸlem gÃ¶rÃ¼yor"
        },
        {
            "question": "RSI nedir?", 
            "context": "RSI momentum gÃ¶stergesidir. 70 Ã¼stÃ¼ aÅŸÄ±rÄ± alÄ±m.",
            "answer": "momentum gÃ¶stergesidir"
        },
        {
            "question": "BIST 100 nasÄ±l?",
            "context": "BIST 100 endeksi %1.25 yÃ¼kseliÅŸle kapandÄ±.",
            "answer": "%1.25 yÃ¼kseliÅŸle"
        }
    ]
    
    print(f"âœ… {len(training_data)} training samples ready")
    
    # Simple preprocessing
    def simple_preprocess(data):
        processed = []
        
        for item in data:
            encoded = tokenizer(
                item["question"],
                item["context"], 
                max_length=256,
                truncation=True,
                padding="max_length",
                return_tensors="pt"
            )
            
            # Simple answer positioning
            context = item["context"]
            answer = item["answer"]
            start_char = context.find(answer)
            
            if start_char >= 0:
                start_pos = max(1, len(context[:start_char].split()))
                end_pos = min(start_pos + len(answer.split()) - 1, 250)
            else:
                start_pos, end_pos = 1, 2
            
            processed.append({
                'input_ids': encoded['input_ids'].squeeze(),
                'attention_mask': encoded['attention_mask'].squeeze(), 
                'start_positions': torch.tensor(start_pos, dtype=torch.long),
                'end_positions': torch.tensor(end_pos, dtype=torch.long)
            })
            
        return processed
    
    try:
        print("Processing training data...")
        processed_data = simple_preprocess(training_data)
        print(f"âœ… {len(processed_data)} samples processed")
        
        # Create dataset
        from datasets import Dataset
        dataset_dict = {
            'input_ids': [item['input_ids'] for item in processed_data],
            'attention_mask': [item['attention_mask'] for item in processed_data],
            'start_positions': [item['start_positions'] for item in processed_data], 
            'end_positions': [item['end_positions'] for item in processed_data]
        }
        
        train_dataset = Dataset.from_dict(dataset_dict)
        print("âœ… Dataset created!")
        
        # Training setup
        from transformers import TrainingArguments, Trainer, DefaultDataCollator
        from datetime import datetime
        
        training_args = TrainingArguments(
            output_dir="./fixed-turkish-qa",
            learning_rate=2e-5,
            num_train_epochs=2,
            per_device_train_batch_size=1,
            gradient_accumulation_steps=2,
            warmup_steps=2,
            save_steps=50,
            logging_steps=1,
            push_to_hub=True,
            hub_model_id=HF_MODEL_NAME,
            fp16=torch.cuda.is_available(),
            dataloader_pin_memory=False,
        )
        
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            tokenizer=tokenizer,
            data_collator=DefaultDataCollator(),
        )
        
        print("ğŸš€ Starting minimal training...")
        print(f"â° Start: {datetime.now().strftime('%H:%M:%S')}")
        
        # Train
        train_result = trainer.train()
        
        print("ğŸ‰ TRAINING SUCCESSFUL!")
        print(f"ğŸ“Š Loss: {train_result.training_loss:.4f}")
        print(f"â° End: {datetime.now().strftime('%H:%M:%S')}")
        
        # Test the model
        from transformers import pipeline
        qa_pipeline = pipeline(
            "question-answering",
            model=trainer.model,
            tokenizer=tokenizer,
            device=0 if torch.cuda.is_available() else -1
        )
        
        result = qa_pipeline(
            question="GARAN hissesi nasÄ±l?",
            context="GARAN hissesi %-0.94 dÃ¼ÅŸÃ¼ÅŸle iÅŸlem gÃ¶rÃ¼yor."
        )
        
        print(f"âœ… Test: {result['answer']}")
        print(f"ğŸ¯ Score: {result['score']:.3f}")
        
        # Upload
        trainer.push_to_hub(commit_message="Fixed Turkish Financial Q&A")
        print("ğŸ‰ MODEL UPLOADED!")
        print(f"ğŸ“ URL: https://huggingface.co/{HF_MODEL_NAME}")
        
    except Exception as e:
        print(f"âŒ Training error: {e}")

else:
    print("\nâŒ VALIDATION FAILED - FALLBACK OPTIONS:")
    print("1. Restart runtime and try again")
    print("2. Try different approach (clean slate)")  
    print("3. Use existing pre-trained models")

print("\n" + "=" * 60)
if validation_passed:
    print("ğŸ‰ SUCCESS! FORCED FIX WORKED!")
    print("âœ… All imports working")
    print("âœ… Turkish Q&A model trained") 
    print("âœ… Ready for Railway integration")
else:
    print("âš ï¸ FORCED FIX NEEDS ADJUSTMENT")
    print("Check error messages above for next steps")
    
print("=" * 60)

# Document working versions for future
if validation_passed:
    print("\nğŸ“‹ WORKING VERSION COMBINATION (Save this!):")
    print("transformers==4.35.0")
    print("huggingface_hub==0.17.3") 
    print("accelerate==0.24.0")
    print("datasets==2.14.0")
    print("âœ… This combination is tested and working!")
