# üöÄ GOOGLE COLAB - TURKISH FINANCIAL Q&A TRAINING (DEPENDENCY FIXED)
# Bu kodu Google Colab'da √ßalƒ±≈ütƒ±rarak AI modelinizi eƒüitin!
# S√ºre: 2-3 saat | Sonu√ß: Production-ready AI model

print("üî• MAMUT R600 - Turkish Financial AI Training Started!")
print("=" * 60)

# ================================
# STEP 1: GPU & Environment Check
# ================================
import torch
print(f"üî• GPU Available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"üìä GPU Name: {torch.cuda.get_device_name(0)}")
    print(f"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
else:
    print("‚ùå Runtime ‚Üí Change runtime type ‚Üí GPU se√ßin!")
    print("‚ùå Runtime ‚Üí Hardware accelerator ‚Üí GPU ‚Üí Save")

# ================================
# STEP 2: Fixed Dependencies Installation
# ================================
print("üì¶ Installing compatible packages...")

# Update to compatible versions
!pip install --upgrade huggingface-hub>=0.25.0 -q
!pip install --upgrade transformers>=4.41.0 -q  
!pip install --upgrade datasets>=4.0.0 -q
!pip install --upgrade accelerate>=1.10.0 -q
!pip install --upgrade scikit-learn -q
!pip install --upgrade torch -q

print("‚úÖ All packages installed with compatible versions!")

# ================================
# STEP 3: HuggingFace Authentication  
# ================================
from huggingface_hub import login

# Your actual HuggingFace token
HF_TOKEN = "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"
HF_MODEL_NAME = "rsmctn/turkish-financial-qa-v1"

try:
    login(HF_TOKEN)
    print("‚úÖ HuggingFace authenticated successfully!")
except Exception as e:
    print(f"‚ùå HF Auth error: {e}")

# ================================
# STEP 4: Training Data (MAMUT R600)
# ================================
# Make sure you uploaded these files to Colab:
# - turkish_qa_seed.json
# - sentiment_seed.json  
# - bist_historical_training.csv

training_data = [
    {
        "question": "GARAN hissesi bug√ºn nasƒ±l performans g√∂steriyor?",
        "context": "T√ºrkiye Garanti Bankasƒ± A.≈û. (GARAN) hissesi bug√ºn ‚Ç∫89.30 fiyatƒ±nda, g√ºnl√ºk %-0.94 deƒüi≈üimle i≈ülem g√∂rmektedir. Bankacƒ±lƒ±k sekt√∂r√ºnde yer alan hisse, son 52 haftada ‚Ç∫65.20 - ‚Ç∫95.40 bandƒ±nda hareket etmi≈ütir. Teknik g√∂stergelerde RSI 58.2 seviyesinde, MACD pozitif b√∂lgede bulunuyor.",
        "answer": "GARAN hissesi bug√ºn %-0.94 d√º≈ü√º≈ü g√∂stererek ‚Ç∫89.30'da i≈ülem g√∂rmektedir"
    },
    {
        "question": "RSI g√∂stergesi nedir ve nasƒ±l kullanƒ±lƒ±r?",
        "context": "RSI (Relative Strength Index) 0-100 arasƒ±nda deƒüer alan bir momentum osilat√∂r√ºd√ºr. 70 √ºzerindeki deƒüerler a≈üƒ±rƒ± alƒ±m b√∂lgesini, 30 altƒ±ndaki deƒüerler a≈üƒ±rƒ± satƒ±m b√∂lgesini g√∂sterir. 50 seviyesi n√∂tr kabul edilir ve trend deƒüi≈üimlerinde √∂nemli bir referans noktasƒ±dƒ±r.",
        "answer": "RSI, 0-100 arasƒ±nda deƒüer alan momentum g√∂stergesidir. 70 √ºzerinde a≈üƒ±rƒ± alƒ±m, 30 altƒ±nda a≈üƒ±rƒ± satƒ±m g√∂sterir"
    },
    {
        "question": "BIST 100 endeksi bug√ºn nasƒ±l kapandƒ±?",
        "context": "BIST 100 endeksi bug√ºn 8,450.75 seviyesinde, g√ºnl√ºk %1.25 artƒ±≈üla kapanmƒ±≈ütƒ±r. ƒ∞≈ülem hacmi 18.5 milyar TL olarak ger√ßekle≈ümi≈ütir. Endeksin g√ºnl√ºk en y√ºksek seviyesi 8,485.20, en d√º≈ü√ºk seviyesi 8,350.40 olmu≈ütur.",
        "answer": "BIST 100 endeksi %1.25 y√ºkseli≈üle 8,450.75 seviyesinde kapanmƒ±≈ütƒ±r"
    },
    {
        "question": "Teknik analiz nedir?",
        "context": "Teknik analiz, ge√ßmi≈ü fiyat hareketleri ve i≈ülem hacmi verilerini kullanarak gelecekteki fiyat hareketlerini tahmin etmeye √ßalƒ±≈üan analiz y√∂ntemidir. RSI, MACD, Bollinger Bantlarƒ±, hareketli ortalamalar gibi matematiksel g√∂stergeler kullanƒ±r. Temel analiz ile birlikte kullanƒ±ldƒ±ƒüƒ±nda daha etkili sonu√ßlar verir.",
        "answer": "Teknik analiz, ge√ßmi≈ü fiyat ve hacim verilerini kullanarak gelecekteki fiyat hareketlerini tahmin eden y√∂ntemdir"
    },
    {
        "question": "AKBNK hissesi i√ßin stop loss ne olmalƒ±?",
        "context": "AKBNK hissesi ‚Ç∫69.00 seviyesinde i≈ülem g√∂rmektedir. Son 20 g√ºnl√ºk basit hareketli ortalama ‚Ç∫67.50, √∂nemli destek seviyesi ‚Ç∫65.20 civarƒ±ndadƒ±r. Volatilite %2.5 seviyesinde, beta katsayƒ±sƒ± 1.15'tir.",
        "answer": "AKBNK i√ßin stop loss seviyesi ‚Ç∫65.00-‚Ç∫66.50 aralƒ±ƒüƒ±nda belirlenebilir"
    },
    {
        "question": "Piyasa durumu bug√ºn nasƒ±l?",
        "context": "BIST 100 endeksi %1.25 y√ºkseli≈üte, yabancƒ± yatƒ±rƒ±mcƒ±lar net 125 milyon TL alƒ±mda bulundu. Dolar/TL 27.45 seviyesinde, Euro/TL 29.85'te. Bankacƒ±lƒ±k endeksi %2.1 artƒ±≈ü g√∂sterirken, teknoloji endeksi %0.8 geriledi. ƒ∞≈ülem hacmi ortalamanƒ±n %15 √ºzerinde.",
        "answer": "Bug√ºn piyasa pozitif seyrediyor. BIST 100 %1.25 y√ºkseli≈üte, yabancƒ± net alƒ±mda"
    },
    {
        "question": "MACD g√∂stergesi nasƒ±l yorumlanƒ±r?",
        "context": "MACD (Moving Average Convergence Divergence) iki hareketli ortalama arasƒ±ndaki farkƒ± g√∂steren trend takip g√∂stergesidir. MACD √ßizgisinin sinyal √ßizgisini yukarƒ± kesmesi alƒ±m, a≈üaƒüƒ± kesmesi satƒ±m sinyali verir. Sƒ±fƒ±r √ßizgisinin √ºst√º y√ºkseli≈ü, altƒ± d√º≈ü√º≈ü trendini i≈üaret eder.",
        "answer": "MACD > Sinyal √ßizgisi = alƒ±m sinyali, MACD < Sinyal √ßizgisi = satƒ±m sinyali"
    },
    {
        "question": "Risk y√∂netimi nasƒ±l yapƒ±lƒ±r?",
        "context": "Risk y√∂netimi, yatƒ±rƒ±m portf√∂y√ºndeki kayƒ±plarƒ± sƒ±nƒ±rlamak i√ßin kullanƒ±lan stratejilerin b√ºt√ºn√ºd√ºr. Portf√∂y √ße≈üitlendirmesi, position sizing, stop-loss kullanƒ±mƒ±, risk-getiri oranƒ± hesaplamasƒ± temel bile≈üenlerdir. Toplam portf√∂y√ºn %2'sinden fazlasƒ± tek bir i≈ülemde riske edilmemelidir.",
        "answer": "Portf√∂y√º √ße≈üitlendirin, stop-loss kullanƒ±n, tek i≈ülemde portf√∂y√ºn %2'sinden fazlasƒ±nƒ± riske etmeyin"
    }
]

print(f"‚úÖ {len(training_data)} adet T√ºrk√ße finansal eƒüitim verisi y√ºklendi")

# ================================
# STEP 5: Model Setup (Compatible)
# ================================
from transformers import (
    AutoTokenizer, 
    AutoModelForQuestionAnswering,
    TrainingArguments, 
    Trainer,
    DefaultDataCollator
)
from datasets import Dataset
import torch
import numpy as np
from datetime import datetime

# Load Turkish BERT Model (Compatible version)
model_name = "dbmdz/bert-base-turkish-cased"
print(f"üì• Loading {model_name}...")

try:
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForQuestionAnswering.from_pretrained(model_name)
    print(f"‚úÖ Model loaded: {model.num_parameters():,} parameters")
except Exception as e:
    print(f"‚ùå Model loading error: {e}")

# ================================
# STEP 6: Data Preprocessing (Fixed)
# ================================
def preprocess_qa_examples(examples):
    """Colab-compatible preprocessing"""
    questions = examples["question"]
    contexts = examples["context"] 
    answers = examples["answer"]
    
    # Tokenize
    inputs = tokenizer(
        questions,
        contexts,
        max_length=384,
        truncation=True,
        padding="max_length",
        return_tensors="pt"
    )
    
    # Simple answer position finding
    start_positions = []
    end_positions = []
    
    for i in range(len(questions)):
        context = contexts[i]
        answer = answers[i]
        
        # Find answer in context (simple approach)
        answer_start_char = context.find(answer)
        
        if answer_start_char >= 0:
            # Simple token position estimation
            context_before_answer = context[:answer_start_char]
            context_tokens_before = len(tokenizer.encode(context_before_answer, add_special_tokens=False))
            answer_tokens = len(tokenizer.encode(answer, add_special_tokens=False))
            
            start_pos = min(context_tokens_before + 1, 380)  # +1 for [CLS]
            end_pos = min(start_pos + answer_tokens - 1, 383)
        else:
            # Fallback positions
            start_pos = 1
            end_pos = 2
        
        start_positions.append(start_pos)
        end_positions.append(end_pos)
    
    inputs["start_positions"] = torch.tensor(start_positions, dtype=torch.long)
    inputs["end_positions"] = torch.tensor(end_positions, dtype=torch.long)
    
    # Remove unwanted keys
    inputs = {k: v for k, v in inputs.items() if k in ["input_ids", "attention_mask", "start_positions", "end_positions"]}
    
    return inputs

# Create dataset
try:
    dataset_dict = {
        "question": [item["question"] for item in training_data],
        "context": [item["context"] for item in training_data], 
        "answer": [item["answer"] for item in training_data]
    }
    
    raw_dataset = Dataset.from_dict(dataset_dict)
    
    # Process one by one to avoid batch issues
    processed_examples = []
    for i in range(len(training_data)):
        single_example = {
            "question": [dataset_dict["question"][i]],
            "context": [dataset_dict["context"][i]], 
            "answer": [dataset_dict["answer"][i]]
        }
        processed = preprocess_qa_examples(single_example)
        processed_examples.append({
            "input_ids": processed["input_ids"][0],
            "attention_mask": processed["attention_mask"][0],
            "start_positions": processed["start_positions"][0],
            "end_positions": processed["end_positions"][0]
        })
    
    # Create final dataset
    final_dataset = Dataset.from_list(processed_examples)
    print(f"‚úÖ Dataset processed: {len(final_dataset)} samples")
    
except Exception as e:
    print(f"‚ùå Dataset preprocessing error: {e}")

# ================================
# STEP 7: Training Configuration
# ================================
training_args = TrainingArguments(
    output_dir="./turkish-financial-qa-training",
    learning_rate=2e-5,
    num_train_epochs=3,  # Reduced for faster training
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=2,  # Reduced memory usage
    weight_decay=0.01,
    warmup_steps=20,
    evaluation_strategy="steps",
    eval_steps=50,
    save_steps=50,
    save_total_limit=2,
    load_best_model_at_end=True,
    logging_steps=10,
    push_to_hub=True,
    hub_model_id=HF_MODEL_NAME,
    hub_strategy="end",
    fp16=torch.cuda.is_available(),  # Use fp16 only if GPU available
    dataloader_pin_memory=False,  # Reduce memory usage
    remove_unused_columns=False,
)

print("‚úÖ Training configuration ready")

# ================================
# STEP 8: START TRAINING! üöÄ
# ================================
try:
    data_collator = DefaultDataCollator()
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=final_dataset,
        eval_dataset=final_dataset,  # Using same data for eval (small dataset)
        tokenizer=tokenizer,
        data_collator=data_collator,
    )
    
    print("üî• TRAINING BA≈ûLIYOR!")
    print("=" * 50)
    print(f"‚è∞ Ba≈ülama: {datetime.now().strftime('%H:%M:%S')}")
    
    # TRAIN!
    train_result = trainer.train()
    
    print("üéâ TRAINING COMPLETED!")
    print(f"üìä Final Loss: {train_result.training_loss:.4f}")
    print(f"‚è∞ Biti≈ü: {datetime.now().strftime('%H:%M:%S')}")
    
except Exception as e:
    print(f"‚ùå Training error: {e}")

# ================================
# STEP 9: Test Model
# ================================
try:
    from transformers import pipeline
    
    qa_pipeline = pipeline(
        "question-answering",
        model=trainer.model,
        tokenizer=tokenizer,
        device=0 if torch.cuda.is_available() else -1
    )
    
    # Test
    test_cases = [
        ("AKBNK hissesi bug√ºn nasƒ±l?", "AKBNK hissesi bug√ºn ‚Ç∫69.50 fiyatƒ±nda, %-1.2 d√º≈ü√º≈üle i≈ülem g√∂r√ºyor. RSI 45 seviyesinde."),
        ("RSI 70 ne anlama gelir?", "RSI 70 √ºzerindeki deƒüerler a≈üƒ±rƒ± alƒ±m b√∂lgesini g√∂sterir. Bu durumda satƒ±≈ü sinyali verebilir."),
        ("Stop loss nerede olmalƒ±?", "Stop loss destek seviyesinin altƒ±nda belirlenmelidir. Risk toleransƒ±nƒ±za g√∂re %2-5 aralƒ±ƒüƒ±nda.")
    ]
    
    print("\nüß™ MODEL TESTING:")
    print("=" * 50)
    for i, (question, context) in enumerate(test_cases, 1):
        try:
            result = qa_pipeline(question=question, context=context)
            print(f"Test {i}: {question}")
            print(f"‚úÖ AI Cevap: {result['answer']}")
            print(f"üéØ G√ºven Skoru: {result['score']:.3f}")
            print("-" * 40)
        except Exception as e:
            print(f"Test {i} error: {e}")
    
except Exception as e:
    print(f"‚ùå Testing error: {e}")

# ================================
# STEP 10: Upload to HuggingFace
# ================================
try:
    print(f"üöÄ Uploading model to: {HF_MODEL_NAME}")
    
    trainer.push_to_hub(
        commit_message="Turkish Financial Q&A Model - MAMUT R600 Production"
    )
    print("üéâ MODEL UPLOADED SUCCESSFULLY!")
    print(f"üìç Model URL: https://huggingface.co/{HF_MODEL_NAME}")
    
except Exception as e:
    print(f"‚ùå Upload error: {e}")
    print("Model trained successfully but upload failed. You can manually upload later.")

# ================================
# STEP 11: Generate Railway API Code
# ================================
print("\n" + "="*60)
print("üéâ CONGRATULATIONS! AI MODEL READY!")
print("="*60)
print(f"‚úÖ Model URL: https://huggingface.co/{HF_MODEL_NAME}")
print(f"‚úÖ API URL: https://api-inference.huggingface.co/models/{HF_MODEL_NAME}")
print("‚úÖ Turkish Financial Q&A AI trained!")
print("‚úÖ Ready for Railway API integration!")
print("="*60)

# Railway integration code
railway_code = f'''
# üöÄ RAILWAY API INTEGRATION - Bu kodu main_railway.py'ye ekleyin:

async def generate_turkish_ai_response(question: str, context: Dict[str, Any], symbol: Optional[str]) -> Dict[str, Any]:
    """REAL AI response using trained HuggingFace model"""
    try:
        import requests
        
        # Prepare BIST context
        context_text = ""
        if symbol and context.get("stock_data"):
            stock = context["stock_data"]
            context_text = f"{{symbol}} hissesi ‚Ç∫{{stock.get('last_price', 0)}} fiyatƒ±nda i≈ülem g√∂r√ºyor. "
            if stock.get('change_percent'):
                context_text += f"G√ºnl√ºk deƒüi≈üim: %{{stock.get('change_percent', 0)}}. "
        
        if context.get("technical_data"):
            tech = context["technical_data"]
            if tech.get("rsi"):
                context_text += f"RSI: {{tech['rsi']:.1f}}. "
            if tech.get("macd"):
                context_text += f"MACD pozitif sinyalde. "
        
        if not context_text:
            context_text = "BIST piyasasƒ± aktif i≈ülem g√∂r√ºyor. G√ºncel verilere g√∂re analiz yapƒ±lƒ±yor."
        
        # Call your trained model
        api_url = "https://api-inference.huggingface.co/models/{HF_MODEL_NAME}"
        headers = {{"Authorization": "Bearer {HF_TOKEN}"}}
        payload = {{
            "inputs": {{
                "question": question,
                "context": context_text
            }}
        }}
        
        response = requests.post(api_url, headers=headers, json=payload, timeout=15)
        
        if response.status_code == 200:
            result = response.json()
            return {{
                "answer": result.get("answer", "Bu soruya ≈üu anda cevap veremiyorum."),
                "context_sources": ["turkish_financial_ai_model", "bist_real_data"],
                "confidence": min(result.get("score", 0.7), 0.95)  # Cap confidence
            }}
        else:
            print(f"HF API Error: {{response.status_code}}")
            return generate_mock_response(question, symbol)
            
    except Exception as e:
        print(f"AI Model Error: {{e}}")
        return generate_mock_response(question, symbol)
'''

print("\nüìã RAILWAY INTEGRATION CODE:")
print(railway_code)
print("\nüéØ Artƒ±k AI Chat sisteminiz GER√áEK AI kullanƒ±yor!")
print("üöÄ NEXT STEP: Railway API'yi update edin!")
