# ðŸš€ **AUTOMATED BIST DATA PIPELINE - SETUP GUIDE**

## ðŸ“‹ **SYSTEM OVERVIEW**

Bu sistem **1200 Excel dosyasÄ±** ile tarihi verileri import edip, **gÃ¼nlÃ¼k basestock.xls** gÃ¼ncellemeleri ile otomatik AI pipeline Ã§alÄ±ÅŸtÄ±rÄ±r!

---

## ðŸ—‚ **DIRECTORY STRUCTURE**

```
MAMUT_R600/
â”œâ”€â”€ automated_data_pipeline.py          # Ana pipeline sistemi
â”œâ”€â”€ data_pipeline_config.json           # KonfigÃ¼rasyon
â”œâ”€â”€ historical_excel_data/               # 1200 Excel dosyalarÄ±nÄ±n yeri
â”‚   â”œâ”€â”€ 2020_01_BIST_data.xlsx
â”‚   â”œâ”€â”€ 2020_02_BIST_data.xlsx
â”‚   â”œâ”€â”€ ... (1200 files total)
â”œâ”€â”€ daily_updates/                       # GÃ¼nlÃ¼k basestock.xls buraya
â”‚   â”œâ”€â”€ basestock.xls                   # Her sabah/akÅŸam gÃ¼ncellenen
â”‚   â””â”€â”€ backups/                        # Otomatik backup'lar
â”œâ”€â”€ data/                               # Database ve cache
â”‚   â”œâ”€â”€ bist_comprehensive.db           # Ana SQLite database
â”‚   â””â”€â”€ processing_cache/
â”œâ”€â”€ logs/                               # System logs
â”‚   â”œâ”€â”€ data_pipeline.log
â”‚   â””â”€â”€ error_logs/
â””â”€â”€ ai_models/                          # Trained AI models
    â”œâ”€â”€ dp_lstm_model.pkl
    â”œâ”€â”€ turkish_qa_model/
    â””â”€â”€ sentiment_model.pkl
```

---

## ðŸ”§ **SETUP ADIMLARI**

### **1ï¸âƒ£ Environment HazÄ±rlÄ±ÄŸÄ±:**

```bash
# Python dependencies install
pip install pandas numpy sqlite3 schedule watchdog openpyxl xlrd asyncio hashlib logging pathlib

# Directories oluÅŸtur
mkdir -p historical_excel_data daily_updates data logs ai_models
mkdir -p daily_updates/backups data/processing_cache logs/error_logs
```

### **2ï¸âƒ£ Historical Data (1200 Excel) HazÄ±rlÄ±ÄŸÄ±:**

```bash
# 1200 Excel dosyanÄ±zÄ± historical_excel_data/ klasÃ¶rÃ¼ne koyun
# Dosya formatlarÄ± desteklenir:
# - .xlsx (Excel 2007+)
# - .xls (Excel 97-2003)

# Expected columns (herhangi bir isimde olabilir):
# - Date/Tarih/DATE
# - Symbol/Sembol/Code
# - Open/AÃ§Ä±lÄ±ÅŸ/AÃ§
# - High/YÃ¼ksek/Max
# - Low/DÃ¼ÅŸÃ¼k/Min  
# - Close/KapanÄ±ÅŸ/Son
# - Volume/Hacim/Miktar
```

### **3ï¸âƒ£ GÃ¼nlÃ¼k Update HazÄ±rlÄ±ÄŸÄ±:**

```bash
# basestock.xls formatÄ±nÄ± daily_updates/ klasÃ¶rÃ¼nde hazÄ±rlayÄ±n
# Sistem otomatik detect edecek ve process edecek

# File formats supported:
# - basestock.xls
# - basestock.xlsx
# - daily_*.xls*
```

---

## ðŸš€ **SISTEM Ã‡ALIÅžTIRMA**

### **Manual Start:**

```bash
cd MAMUT_R600
python automated_data_pipeline.py
```

### **Production Deployment:**

```bash
# Systemd service olarak Ã§alÄ±ÅŸtÄ±r (Linux)
sudo cp bist_pipeline.service /etc/systemd/system/
sudo systemctl enable bist_pipeline
sudo systemctl start bist_pipeline

# Ya da Docker container
docker build -t bist-pipeline .
docker run -d --name bist-pipeline -v ./data:/app/data bist-pipeline
```

### **Background Process:**

```bash
# nohup ile background'da Ã§alÄ±ÅŸtÄ±r
nohup python automated_data_pipeline.py > pipeline.log 2>&1 &

# Screen session ile
screen -S bist_pipeline
python automated_data_pipeline.py
# Ctrl+A, D ile detach
```

---

## ðŸ“Š **SYSTEM FEATURES**

### **ðŸ”„ Automated Processing:**
- **Historical Import**: 1200 Excel dosyasÄ±nÄ± paralel iÅŸleme
- **Daily Monitoring**: basestock.xls deÄŸiÅŸikliklerini real-time izleme  
- **Change Detection**: Hash-based file change detection
- **Database Updates**: Incremental updates, duplicate detection

### **ðŸ“ˆ Technical Analysis:**
- **RSI (14)**: Relative Strength Index
- **MACD**: Moving Average Convergence Divergence
- **Bollinger Bands**: Volatility bands
- **Ichimoku Cloud**: Comprehensive trend analysis
- **Custom Indicators**: Extensible indicator system

### **ðŸ¤– AI Integration:**
- **Auto Retraining**: New data trigger model retraining
- **Prediction Generation**: Multi-horizon price predictions  
- **Model Versioning**: Track different model versions
- **Performance Monitoring**: Model accuracy tracking

### **ðŸ” Monitoring & Logging:**
- **Comprehensive Logs**: All operations logged
- **Performance Metrics**: Processing time, record counts
- **Error Handling**: Graceful error recovery
- **System Health**: Database stats, uptime monitoring

---

## âš¡ **PERFORMANCE OPTIMIZATIONS**

### **Database Optimizations:**
```sql
-- Automatic indexing
CREATE INDEX idx_historical_symbol_date ON historical_data(symbol, date);
CREATE INDEX idx_technical_symbol_date ON technical_indicators(symbol, date);

-- Batch processing
INSERT INTO historical_data VALUES ... (1000 records at once)

-- Query optimizations
SELECT * FROM historical_data WHERE symbol = ? AND date >= ? ORDER BY date;
```

### **Memory Management:**
- **Chunk Processing**: Large files processed in chunks
- **Memory Limits**: 4GB default limit, configurable
- **Cache System**: Intelligent caching for frequent queries
- **Garbage Collection**: Automatic memory cleanup

### **Parallel Processing:**
- **Multi-threading**: Excel files processed in parallel
- **Async Operations**: Database operations asynchronous
- **Queue System**: Processing queue for large batches

---

## ðŸ”§ **CONFIGURATION OPTIONS**

### **Historical Data Settings:**
```json
{
  "historical_data": {
    "parallel_processing": true,
    "max_workers": 4,
    "batch_size": 1000,
    "data_validation": {
      "min_price": 0.01,
      "max_price": 10000,
      "remove_weekends": true
    }
  }
}
```

### **AI Processing Settings:**
```json
{
  "ai_processing": {
    "retrain_threshold": {
      "new_records_count": 100,
      "days_since_last_training": 7
    },
    "prediction_horizon_days": [1, 3, 7, 14, 30],
    "auto_deployment": true
  }
}
```

### **Monitoring Settings:**
```json
{
  "monitoring": {
    "logging_level": "INFO",
    "email_alerts": true,
    "dashboard_port": 8080
  }
}
```

---

## ðŸ“¡ **API ENDPOINTS**

Sistem Ã§alÄ±ÅŸtÄ±ktan sonra HTTP API'ler kullanÄ±labilir:

```bash
# System status
curl http://localhost:8080/api/status

# Database statistics  
curl http://localhost:8080/api/stats

# Recent predictions
curl http://localhost:8080/api/predictions?symbol=GARAN&days=7

# Trigger manual retraining
curl -X POST http://localhost:8080/api/retrain?symbol=AKBNK

# Get technical indicators
curl http://localhost:8080/api/indicators?symbol=THYAO&period=30
```

---

## ðŸŽ¯ **WORKFLOW EXAMPLE**

### **Day 1: Initial Setup**
```
09:00 - 1200 Excel files historical_excel_data/ klasÃ¶rÃ¼ne konur
09:15 - python automated_data_pipeline.py Ã§alÄ±ÅŸtÄ±rÄ±lÄ±r
09:16 - System 1200 dosyayÄ± import etmeye baÅŸlar (parallel processing)
11:30 - Import tamamlanÄ±r: 2.5M historical records imported
11:35 - Technical indicators calculation baÅŸlar
12:45 - All indicators calculated, system ready
```

### **Day 2+: Daily Operations**
```
08:00 - Yeni basestock.xls daily_updates/ klasÃ¶rÃ¼ne konur
08:01 - System otomatik detect eder, processing baÅŸlar
08:02 - 150 new records imported, 50 records updated
08:05 - Technical indicators updated for affected symbols
08:10 - AI models retrained (if threshold met)
08:15 - New predictions generated and stored
08:20 - System ready for queries
```

---

## ðŸš¨ **TROUBLESHOOTING**

### **Common Issues:**

**1. Excel Import Errors:**
```bash
# Check file permissions
ls -la historical_excel_data/

# Test single file
python -c "import pandas as pd; print(pd.read_excel('test.xlsx'))"

# Check column names
python -c "import pandas as pd; print(pd.read_excel('test.xlsx').columns)"
```

**2. Database Lock Errors:**
```bash
# Check database permissions
sqlite3 data/bist_comprehensive.db ".databases"

# Restart with clean database
rm data/bist_comprehensive.db
python automated_data_pipeline.py
```

**3. Memory Issues:**
```bash
# Reduce batch size in config
"batch_size": 500  # Instead of 1000

# Limit parallel workers  
"max_workers": 2   # Instead of 4
```

### **Logs Location:**
```bash
# Main log
tail -f logs/data_pipeline.log

# Error logs
tail -f logs/error_logs/$(date +%Y-%m-%d).log

# System monitoring
htop  # CPU/Memory usage
```

---

## ðŸŽ‰ **EXPECTED RESULTS**

### **After 1200 Excel Import:**
- âœ… **2M+ historical records** in database
- âœ… **500+ unique symbols** tracked  
- âœ… **Technical indicators** calculated for all symbols
- âœ… **Database optimized** with proper indexes
- âœ… **System ready** for daily updates

### **Daily Operations:**
- âœ… **5-10 seconds** basestock.xls processing
- âœ… **Real-time updates** to affected symbols
- âœ… **Auto AI retraining** when thresholds met
- âœ… **New predictions** generated automatically
- âœ… **Zero manual intervention** required

### **Performance Metrics:**
- ðŸ“Š **Import Speed**: ~2000 records/second
- ðŸ”„ **Daily Updates**: <10 seconds end-to-end
- ðŸ¤– **AI Training**: <5 minutes for incremental
- ðŸ’¾ **Memory Usage**: <2GB typical, <4GB peak
- ðŸ—„ï¸ **Database Size**: ~500MB for 2M records

---

## ðŸŽ¯ **SONUÃ‡**

Bu sistem ile:
- **Manual work eliminated** - Tamamen otomatik
- **Real-time processing** - AnÄ±nda gÃ¼ncellemeler  
- **AI-powered insights** - Otomatik tahminler
- **Scalable architecture** - BÃ¼yÃ¼meye hazÄ±r
- **Production-ready** - 7/24 Ã§alÄ±ÅŸmaya hazÄ±r

**1200 Excel + daily basestock.xls = Tamamen automated BIST AI system!** ðŸš€

**Setup tamamlandÄ±ktan sonra sadece basestock.xls'i gÃ¼ncellemek yeterli, geri kalanÄ± otomatik!** âœ…
