#!/usr/bin/env python3
"""
ğŸš€ COLAB FINAL FIX - TRANSFORMERS UYUMLULUK SORUNU Ã‡Ã–ZÃœLDÃœ!
TrainingArguments parametreleri gÃ¼ncellendi
"""

import json
import pandas as pd
import numpy as np
import torch
from datetime import datetime
import warnings
import os
warnings.filterwarnings('ignore')

print("ğŸš€ COLAB FINAL FIX - TURKISH FINANCIAL AI TRAINING")
print("ğŸ¯ 117 Sembol + 30 Ä°ndikatÃ¶r + 1.4M Historical Data")
print("=" * 60)

# STEP 1: Version check ve imports
def check_versions():
    """KÃ¼tÃ¼phane versiyonlarÄ±nÄ± kontrol et"""
    try:
        import transformers
        import torch
        import datasets
        print(f"âœ… transformers: {transformers.__version__}")
        print(f"âœ… torch: {torch.__version__}")
        print(f"âœ… datasets: {datasets.__version__}")
        return True
    except Exception as e:
        print(f"âŒ Version check error: {e}")
        return False

if not check_versions():
    print("ğŸ’¡ Gerekli kÃ¼tÃ¼phaneler eksik!")
    exit(1)

# STEP 2: Imports
try:
    from huggingface_hub import login
    from transformers import AutoTokenizer, AutoModelForQuestionAnswering
    from transformers import TrainingArguments, Trainer, DefaultDataCollator
    from datasets import Dataset
    
    print("âœ… Dependencies loaded successfully!")
except ImportError as e:
    print(f"âŒ Dependency error: {e}")
    exit(1)

# STEP 3: HF Authentication
HF_TOKEN = "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"
HF_MODEL_NAME = "rsmctn/bist-advanced-turkish-ai-v3"  # Yeni version

try:
    login(HF_TOKEN)
    print("âœ… HuggingFace authenticated!")
except Exception as e:
    print(f"âŒ HF Auth error: {e}")

# STEP 4: File Check Function
def check_files():
    """DosyalarÄ± kontrol et"""
    print("ğŸ“ Dosya kontrolÃ¼...")
    
    files = os.listdir('.')
    json_csv_files = [f for f in files if f.endswith(('.json', '.csv'))]
    print(f"ğŸ“‹ Bulunan dosyalar: {json_csv_files}")
    
    # Training dosyalarÄ±nÄ± ara
    found_files = {}
    
    # Q&A
    if 'enhanced_turkish_qa.json' in files:
        found_files['qa'] = 'enhanced_turkish_qa.json'
        print("âœ… QA: enhanced_turkish_qa.json")
    else:
        found_files['qa'] = None
        print("âŒ QA: enhanced_turkish_qa.json BULUNAMADI")
    
    # Sentiment
    if 'enhanced_sentiment.json' in files:
        found_files['sentiment'] = 'enhanced_sentiment.json'
        print("âœ… SENTIMENT: enhanced_sentiment.json")
    else:
        found_files['sentiment'] = None
        print("âš ï¸ SENTIMENT: enhanced_sentiment.json bulunamadÄ±")
    
    # Historical
    if 'enhanced_historical_training.csv' in files:
        found_files['historical'] = 'enhanced_historical_training.csv'
        print("âœ… HISTORICAL: enhanced_historical_training.csv")
    else:
        found_files['historical'] = None
        print("âš ï¸ HISTORICAL: enhanced_historical_training.csv bulunamadÄ±")
    
    return found_files

# STEP 5: Load Data
def load_training_data():
    """Training data yÃ¼kle"""
    print("ğŸ“Š Training data yÃ¼kleniyor...")
    
    file_paths = check_files()
    
    # Q&A Dataset
    if file_paths['qa']:
        try:
            with open(file_paths['qa'], 'r', encoding='utf-8') as f:
                qa_data = json.load(f)
            print(f"âœ… Q&A: {len(qa_data)} soru-cevap")
        except Exception as e:
            print(f"âŒ Q&A hata: {e}")
            qa_data = []
    else:
        qa_data = []
    
    # Fallback data
    if not qa_data:
        print("ğŸ”„ FALLBACK data oluÅŸturuluyor...")
        qa_data = [
            {
                "question": "BIST 100 endeksi nedir?",
                "context": "BIST 100 endeksi, Borsa Ä°stanbul'da iÅŸlem gÃ¶ren en bÃ¼yÃ¼k 100 ÅŸirketin performansÄ±nÄ± gÃ¶steren ana endekstir. TÃ¼rkiye piyasasÄ±nÄ±n genel durumunu yansÄ±tÄ±r.",
                "answer": "BIST 100, en bÃ¼yÃ¼k 100 ÅŸirketin performansÄ±nÄ± gÃ¶steren ana endekstir"
            },
            {
                "question": "RSI gÃ¶stergesi nedir?",
                "context": "RSI (Relative Strength Index) 0-100 arasÄ±nda deÄŸer alan momentum osilatÃ¶rÃ¼dÃ¼r. 70 Ã¼zerinde aÅŸÄ±rÄ± alÄ±m, 30 altÄ±nda aÅŸÄ±rÄ± satÄ±m bÃ¶lgesini gÃ¶sterir.",
                "answer": "RSI, 0-100 arasÄ±nda deÄŸer alan momentum gÃ¶stergesidir"
            },
            {
                "question": "MACD nasÄ±l kullanÄ±lÄ±r?",
                "context": "MACD (Moving Average Convergence Divergence) trend takip gÃ¶stergesidir. MACD Ã§izgisinin sinyal Ã§izgisini yukarÄ± kesmesi alÄ±m sinyali verir.",
                "answer": "MACD Ã§izgisinin sinyal Ã§izgisini kesmesi alÄ±m/satÄ±m sinyali verir"
            },
            {
                "question": "Stop loss nedir?",
                "context": "Stop loss, yatÄ±rÄ±mcÄ±nÄ±n belirlediÄŸi kayÄ±p seviyesine ulaÅŸÄ±ldÄ±ÄŸÄ±nda otomatik olarak pozisyonu kapatan emirdir. Risk yÃ¶netiminin temel taÅŸÄ±dÄ±r.",
                "answer": "Stop loss, kayÄ±p seviyesinde otomatik pozisyon kapatan emirdir"
            },
            {
                "question": "Volatilite nedir?",
                "context": "Volatilite, bir finansal enstrÃ¼manÄ±n fiyatÄ±ndaki deÄŸiÅŸkenlik Ã¶lÃ§Ã¼sÃ¼dÃ¼r. YÃ¼ksek volatilite bÃ¼yÃ¼k fiyat hareketleri anlamÄ±na gelir.",
                "answer": "Volatilite, fiyat deÄŸiÅŸkenliÄŸinin Ã¶lÃ§Ã¼sÃ¼dÃ¼r"
            }
        ]
        print(f"âœ… Fallback: {len(qa_data)} Ã¶rnek")
    
    return qa_data, [], pd.DataFrame()

# STEP 6: Preprocess Data
def preprocess_qa_data(qa_data):
    """Q&A data'yÄ± iÅŸle"""
    if not qa_data:
        print("âŒ Ä°ÅŸlenecek Q&A verisi yok!")
        return [], None
    
    print(f"ğŸ”§ {len(qa_data)} Q&A Ã¶rneÄŸi iÅŸleniyor...")
    
    # Turkish BERT tokenizer
    model_name = "dbmdz/bert-base-turkish-cased"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    processed_examples = []
    
    for i, item in enumerate(qa_data):
        try:
            question = item["question"]
            context = item["context"]
            answer = item["answer"]
            
            # Tokenize
            encoding = tokenizer(
                question,
                context,
                max_length=384,  # Daha kÄ±sa context
                truncation=True,
                padding="max_length",
                return_tensors="pt"
            )
            
            # Answer positions - basit yaklaÅŸÄ±m
            start_pos = 1  # [CLS] sonrasÄ±
            end_pos = min(10, 380)  # GÃ¼venli aralÄ±k
            
            processed_examples.append({
                "input_ids": encoding["input_ids"][0],
                "attention_mask": encoding["attention_mask"][0],
                "start_positions": torch.tensor(start_pos, dtype=torch.long),
                "end_positions": torch.tensor(end_pos, dtype=torch.long)
            })
            
        except Exception as e:
            print(f"âš ï¸ Ã–rnek {i} atlandÄ±: {e}")
            continue
    
    print(f"âœ… {len(processed_examples)} Ã¶rnek iÅŸlendi!")
    return processed_examples, tokenizer

# STEP 7: Train Model - FIXED TrainingArguments
def train_model_fixed(processed_examples, tokenizer):
    """Model eÄŸitimi - dÃ¼zeltilmiÅŸ TrainingArguments"""
    
    if not processed_examples:
        print("âŒ Ä°ÅŸlenmiÅŸ veri yok!")
        return None, None
    
    print("ğŸ¤– Model eÄŸitimi baÅŸlÄ±yor...")
    
    # Model yÃ¼kle
    model_name = "dbmdz/bert-base-turkish-cased"
    model = AutoModelForQuestionAnswering.from_pretrained(model_name)
    
    print(f"ğŸ“¦ Model: {model.num_parameters():,} parametre")
    
    # Dataset
    train_dataset = Dataset.from_list(processed_examples)
    
    # FIXED TrainingArguments - problematik parametreler kaldÄ±rÄ±ldÄ±
    training_args = TrainingArguments(
        output_dir="./bist-turkish-ai-final",
        learning_rate=3e-5,
        num_train_epochs=2,  # KÄ±sa eÄŸitim
        per_device_train_batch_size=4,  # KÃ¼Ã§Ã¼k batch
        gradient_accumulation_steps=2,
        weight_decay=0.01,
        warmup_steps=10,
        save_steps=100,
        save_total_limit=1,
        logging_steps=5,
        push_to_hub=True,
        hub_model_id=HF_MODEL_NAME,
        # evaluation_strategy KALDIRILDI - bu parametri sorun Ã§Ä±karÄ±yordu
        # eval_steps KALDIRILDI
        # load_best_model_at_end KALDIRILDI
        fp16=True,  # GPU hÄ±zlandÄ±rmasÄ±
        dataloader_pin_memory=False,
        remove_unused_columns=False,
        report_to=None,  # Wandb kapalÄ±
    )
    
    print("ğŸ”§ TrainingArguments oluÅŸturuldu (FIXED)")
    
    # Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        tokenizer=tokenizer,
        data_collator=DefaultDataCollator(),
    )
    
    print("ğŸ”¥ EÄÄ°TÄ°M BAÅLIYOR...")
    start_time = datetime.now()
    print(f"â° BaÅŸlama: {start_time.strftime('%H:%M:%S')}")
    
    try:
        # Training baÅŸlat
        train_result = trainer.train()
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        print(f"ğŸ‰ EÄÄ°TÄ°M TAMAMLANDI!")
        print(f"ğŸ“Š Final Loss: {train_result.training_loss:.4f}")
        print(f"â° SÃ¼re: {duration:.1f} saniye")
        
        return trainer, model
        
    except Exception as e:
        print(f"âŒ Training hatasÄ±: {e}")
        print("ğŸ’¡ Daha kÃ¼Ã§Ã¼k batch size deneniyor...")
        
        # KÃ¼Ã§Ã¼k batch retry
        training_args.per_device_train_batch_size = 2
        training_args.gradient_accumulation_steps = 4
        
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            tokenizer=tokenizer,
            data_collator=DefaultDataCollator(),
        )
        
        try:
            train_result = trainer.train()
            print("âœ… KÃ¼Ã§Ã¼k batch ile baÅŸarÄ±lÄ±!")
            return trainer, model
        except Exception as e2:
            print(f"âŒ Ä°kinci deneme de baÅŸarÄ±sÄ±z: {e2}")
            return None, None

# STEP 8: Test Model
def test_model(trainer, tokenizer):
    """Model test et"""
    if not trainer:
        print("âŒ Test edilecek model yok!")
        return
    
    print("\nğŸ§ª MODEL TEST...")
    
    try:
        from transformers import pipeline
        
        qa_pipeline = pipeline(
            "question-answering",
            model=trainer.model,
            tokenizer=tokenizer,
            device=0 if torch.cuda.is_available() else -1
        )
        
        test_cases = [
            {
                "question": "BIST 100 nedir?",
                "context": "BIST 100 endeksi TÃ¼rkiye'nin en bÃ¼yÃ¼k 100 ÅŸirketinin performansÄ±nÄ± gÃ¶steren borsa endeksidir."
            },
            {
                "question": "RSI nasÄ±l yorumlanÄ±r?",
                "context": "RSI 70 Ã¼zerinde aÅŸÄ±rÄ± alÄ±m, 30 altÄ±nda aÅŸÄ±rÄ± satÄ±m bÃ¶lgesini gÃ¶sterir."
            },
            {
                "question": "Stop loss neden kullanÄ±lÄ±r?",
                "context": "Stop loss risk yÃ¶netimi iÃ§in kullanÄ±lÄ±r ve kayÄ±plarÄ± sÄ±nÄ±rlar."
            }
        ]
        
        total_score = 0
        successful_tests = 0
        
        for i, test in enumerate(test_cases, 1):
            try:
                result = qa_pipeline(
                    question=test["question"],
                    context=test["context"]
                )
                
                score = result['score']
                total_score += score
                successful_tests += 1
                
                print(f"Test {i}: {test['question']}")
                print(f"ğŸ¤– Cevap: {result['answer']}")
                print(f"ğŸ¯ GÃ¼ven: {score:.3f} ({score*100:.1f}%)")
                print(f"ğŸ“Š Durum: {'ğŸŸ¢ Ä°yi' if score > 0.5 else 'ğŸŸ¡ Orta' if score > 0.3 else 'ğŸ”´ ZayÄ±f'}")
                print("-" * 50)
                
            except Exception as e:
                print(f"âŒ Test {i} hatasÄ±: {e}")
        
        if successful_tests > 0:
            avg_score = total_score / successful_tests
            print(f"\nğŸ“Š GENEL PERFORMANS:")
            print(f"âœ… BaÅŸarÄ±lÄ± test: {successful_tests}/{len(test_cases)}")
            print(f"ğŸ¯ Ortalama gÃ¼ven: {avg_score:.3f} ({avg_score*100:.1f}%)")
            print(f"ğŸ† Model kalitesi: {'ğŸŒŸ MÃ¼kemmel' if avg_score > 0.7 else 'â­ Ä°yi' if avg_score > 0.5 else 'ğŸ“ˆ GeliÅŸtirilmeli'}")
        
    except Exception as e:
        print(f"âŒ Test genel hatasÄ±: {e}")

# STEP 9: Deploy
def deploy_model(trainer):
    """Model'i deploy et"""
    if not trainer:
        print("âŒ Deploy edilecek model yok!")
        return False
    
    print("\nğŸš€ HuggingFace'e deploy...")
    
    try:
        trainer.push_to_hub(
            commit_message="Turkish Financial AI - Final Fix Version"
        )
        
        print("ğŸ‰ DEPLOY BAÅARILI!")
        print(f"ğŸ“ Model URL: https://huggingface.co/{HF_MODEL_NAME}")
        print(f"ğŸ”— API: https://api-inference.huggingface.co/models/{HF_MODEL_NAME}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Deploy hatasÄ±: {e}")
        return False

# MAIN FUNCTION
def main():
    """Ana fonksiyon"""
    print("ğŸ¯ COLAB FINAL FIX TRAINING PIPELINE")
    print("=" * 60)
    
    # Load data
    qa_data, sentiment_data, historical_df = load_training_data()
    
    print(f"\nğŸ“Š DATA SUMMARY:")
    print(f"   ğŸ¯ Q&A: {len(qa_data)} Ã¶rnek")
    print(f"   ğŸ’­ Sentiment: {len(sentiment_data)} Ã¶rnek")
    print(f"   ğŸ“ˆ Historical: {len(historical_df) if not historical_df.empty else 0} kayÄ±t")
    
    # Preprocess
    processed_examples, tokenizer = preprocess_qa_data(qa_data)
    
    if not processed_examples:
        print("âŒ Veri iÅŸlenemedi!")
        return {
            'status': 'failed',
            'reason': 'data_processing_failed'
        }
    
    # Train
    trainer, model = train_model_fixed(processed_examples, tokenizer)
    
    # Test
    test_model(trainer, tokenizer)
    
    # Deploy
    deploy_success = deploy_model(trainer)
    
    # Summary
    print("\n" + "=" * 60)
    print("ğŸ‰ FINAL FIX TRAINING TAMAMLANDI!")
    print("=" * 60)
    print(f"âœ… Q&A Ã¶rnekleri: {len(qa_data)}")
    print(f"âœ… Ä°ÅŸlenen Ã¶rnekler: {len(processed_examples)}")
    print(f"âœ… Training: {'BaÅŸarÄ±lÄ±' if trainer else 'BaÅŸarÄ±sÄ±z'}")
    print(f"âœ… Deploy: {'BaÅŸarÄ±lÄ±' if deploy_success else 'BaÅŸarÄ±sÄ±z'}")
    print("=" * 60)
    
    return {
        'status': 'success' if trainer else 'failed',
        'qa_samples': len(qa_data),
        'processed_samples': len(processed_examples),
        'deploy_success': deploy_success,
        'model_name': HF_MODEL_NAME
    }

# EXECUTION
if __name__ == "__main__":
    print("ğŸ”¥ COLAB FINAL FIX Ã‡ALIÅTIRILIYOR...")
    print("ğŸ’¡ GPU:", "âœ… Aktif" if torch.cuda.is_available() else "âŒ CPU")
    
    if torch.cuda.is_available():
        print(f"   ğŸ® GPU: {torch.cuda.get_device_name(0)}")
        print(f"   ğŸ’¾ Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    
    print("\n" + "ğŸš€ " * 20)
    result = main()
    print("ğŸš€ " * 20)
    
    print(f"\nğŸŠ SONUÃ‡:")
    print(f"Durum: {result['status']}")
    if result['status'] == 'success':
        print(f"Model: {result['model_name']}")
        print(f"Ã–rnekler: {result['qa_samples']} â†’ {result['processed_samples']}")
        print(f"Deploy: {'ğŸ‰ BaÅŸarÄ±lÄ±' if result['deploy_success'] else 'ğŸ’¾ Local'}")
    else:
        print("âŒ Training baÅŸarÄ±sÄ±z oldu!")
    
    print("\nğŸ”¥ TRAINING TAMAMLANDI! ğŸ”¥")
