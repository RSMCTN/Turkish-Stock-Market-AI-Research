#!/usr/bin/env python3
"""
ğŸš€ COLAB ULTIMATE FIX - TÃœM VERÄ°LER YÃœKLENIYOR!
Sentiment + Historical + Q&A = FULL DATASET
"""

import json
import pandas as pd
import numpy as np
import torch
from datetime import datetime
import warnings
import os
warnings.filterwarnings('ignore')

print("ğŸš€ COLAB ULTIMATE FIX - FULL DATASET TRAINING!")
print("ğŸ¯ 117 Sembol + 30 Ä°ndikatÃ¶r + TÃœM VERÄ°LER")
print("=" * 60)

# STEP 1: Imports ve versiyonlar
try:
    import transformers
    print(f"âœ… transformers: {transformers.__version__}")
    
    from huggingface_hub import login
    from transformers import AutoTokenizer, AutoModelForQuestionAnswering
    from transformers import TrainingArguments, Trainer, DefaultDataCollator
    from datasets import Dataset
    
    print("âœ… Dependencies loaded!")
except ImportError as e:
    print(f"âŒ Import error: {e}")
    exit(1)

# HF Setup
HF_TOKEN = "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"
HF_MODEL_NAME = "rsmctn/bist-ultimate-turkish-ai-v4"  # Ultimate version

try:
    login(HF_TOKEN)
    print("âœ… HuggingFace authenticated!")
except Exception as e:
    print(f"âŒ Auth error: {e}")

# STEP 2: FULL Data Loading - FIXED!
def load_full_training_data():
    """TÃœM training data'yÄ± yÃ¼kle - Q&A + Sentiment + Historical"""
    
    print("ğŸ“Š FULL DATASET yÃ¼kleniyor...")
    
    # DosyalarÄ± kontrol et
    files = os.listdir('.')
    print(f"ğŸ“‚ Dosyalar: {[f for f in files if f.endswith(('.json', '.csv'))]}")
    
    # 1. Q&A Dataset
    qa_data = []
    if 'enhanced_turkish_qa.json' in files:
        try:
            with open('enhanced_turkish_qa.json', 'r', encoding='utf-8') as f:
                qa_data = json.load(f)
            print(f"âœ… Q&A: {len(qa_data)} soru-cevap")
        except Exception as e:
            print(f"âŒ Q&A error: {e}")
    
    # 2. Sentiment Dataset - FIXED!
    sentiment_data = []
    if 'enhanced_sentiment.json' in files:
        try:
            with open('enhanced_sentiment.json', 'r', encoding='utf-8') as f:
                sentiment_data = json.load(f)
            print(f"âœ… SENTIMENT: {len(sentiment_data)} Ã¶rnek")
        except Exception as e:
            print(f"âŒ Sentiment error: {e}")
    
    # 3. Historical Dataset - FIXED!
    historical_df = pd.DataFrame()
    if 'enhanced_historical_training.csv' in files:
        try:
            historical_df = pd.read_csv('enhanced_historical_training.csv')
            print(f"âœ… HISTORICAL: {len(historical_df)} kayÄ±t")
            if 'symbol' in historical_df.columns:
                print(f"ğŸ“Š Semboller: {historical_df['symbol'].nunique()} benzersiz")
            if 'date' in historical_df.columns:
                print(f"ğŸ“ˆ Tarih: {historical_df['date'].min()} â†’ {historical_df['date'].max()}")
        except Exception as e:
            print(f"âŒ Historical error: {e}")
    
    # Sentiment'i Q&A formatÄ±na Ã§evir
    if sentiment_data:
        print("ğŸ”„ Sentiment data â†’ Q&A formatÄ±na Ã§eviriliyor...")
        for item in sentiment_data[:50]:  # Ä°lk 50 Ã¶rnek
            sentiment_score = item.get('score', 0)
            sentiment_label = item.get('sentiment', 'neutral')
            text = item.get('text', '')
            
            qa_item = {
                "question": f"{text} - bu haberin sentiment analizi nedir?",
                "context": f"'{text}' haberi finansal piyasalar iÃ§in {sentiment_label} bir geliÅŸmedir. Sentiment skoru {sentiment_score:.2f} olarak hesaplanmÄ±ÅŸtÄ±r. Bu analiz haber metninin olumlu, olumsuz veya nÃ¶tr etkisini gÃ¶sterir.",
                "answer": f"Bu haberin sentiment'i {sentiment_label}, skorun {sentiment_score:.2f}"
            }
            qa_data.append(qa_item)
        
        print(f"âœ… Sentiment â†’ Q&A: {len(sentiment_data[:50])} Ã¶rnek eklendi")
    
    # Historical'Ä± Q&A formatÄ±na Ã§evir
    if not historical_df.empty:
        print("ğŸ”„ Historical data â†’ Q&A formatÄ±na Ã§eviriliyor...")
        # Ä°lk 30 kayÄ±t al
        for i, row in historical_df.head(30).iterrows():
            try:
                symbol = row.get('symbol', 'UNKNOWN')
                close = row.get('close', 0)
                volume = row.get('volume', 0)
                rsi = row.get('rsi_14', 50)
                date = row.get('date', '2024-01-01')
                
                qa_item = {
                    "question": f"{symbol} hissesinin {date} tarihindeki performansÄ± nedir?",
                    "context": f"{symbol} hissesi {date} tarihinde â‚º{close:.2f} fiyatÄ±nda kapanmÄ±ÅŸtÄ±r. Ä°ÅŸlem hacmi {volume:,.0f} adet olarak gerÃ§ekleÅŸmiÅŸtir. RSI deÄŸeri {rsi:.1f} seviyesindedir. Bu veriler hissenin o gÃ¼nkÃ¼ piyasa performansÄ±nÄ± gÃ¶stermektedir.",
                    "answer": f"{symbol} hissesi {date} tarihinde â‚º{close:.2f} fiyatÄ±nda kapanmÄ±ÅŸtÄ±r"
                }
                qa_data.append(qa_item)
            except Exception as e:
                continue
        
        print(f"âœ… Historical â†’ Q&A: 30 Ã¶rnek eklendi")
    
    # Fallback ekle
    if len(qa_data) < 20:
        print("ğŸ”„ Fallback Q&A ekleniyor...")
        fallback_qa = [
            {
                "question": "BIST 100 endeksi nasÄ±l Ã§alÄ±ÅŸÄ±r?",
                "context": "BIST 100 endeksi, Borsa Ä°stanbul'da iÅŸlem gÃ¶ren en bÃ¼yÃ¼k 100 ÅŸirketin piyasa deÄŸeri aÄŸÄ±rlÄ±klÄ± performansÄ±nÄ± yansÄ±tan ana gÃ¶stergedir. Endeks deÄŸeri ÅŸirketlerin toplam piyasa deÄŸerindeki deÄŸiÅŸimlere gÃ¶re hesaplanÄ±r.",
                "answer": "BIST 100, en bÃ¼yÃ¼k 100 ÅŸirketin piyasa deÄŸeri aÄŸÄ±rlÄ±klÄ± performans endeksidir"
            },
            {
                "question": "RSI indikatÃ¶rÃ¼ hangi durumlarda kullanÄ±lÄ±r?",
                "context": "RSI (Relative Strength Index) momentum osilatÃ¶rÃ¼dÃ¼r ve 0-100 arasÄ± deÄŸerler alÄ±r. 70 Ã¼zerindeki deÄŸerler aÅŸÄ±rÄ± alÄ±m bÃ¶lgesini, 30 altÄ±ndaki deÄŸerler aÅŸÄ±rÄ± satÄ±m bÃ¶lgesini gÃ¶sterir. YatÄ±rÄ±mcÄ±lar bu seviyelerde pozisyon alma kararlarÄ± verir.",
                "answer": "RSI, aÅŸÄ±rÄ± alÄ±m/satÄ±m seviyelerini belirlemek iÃ§in kullanÄ±lan momentum osilatÃ¶rÃ¼dÃ¼r"
            },
            {
                "question": "MACD gÃ¶stergesinin sinyal Ã§izgisi nasÄ±l yorumlanÄ±r?",
                "context": "MACD gÃ¶stergesinde ana Ã§izginin sinyal Ã§izgisini yukarÄ± kesmesi alÄ±m sinyali, aÅŸaÄŸÄ± kesmesi satÄ±m sinyali olarak deÄŸerlendirilir. AyrÄ±ca MACD'nin sÄ±fÄ±r Ã§izgisinin Ã¼stÃ¼nde olmasÄ± yÃ¼kseliÅŸ, altÄ±nda olmasÄ± dÃ¼ÅŸÃ¼ÅŸ trendini gÃ¶sterir.",
                "answer": "MACD sinyal Ã§izgisi kesiÅŸmeleri alÄ±m/satÄ±m sinyalleri verir"
            }
        ]
        qa_data.extend(fallback_qa)
        print(f"âœ… Fallback: {len(fallback_qa)} Ã¶rnek eklendi")
    
    print(f"\nğŸ“Š FINAL DATASET:")
    print(f"   ğŸ¯ Toplam Q&A: {len(qa_data)} Ã¶rnek")
    print(f"   ğŸ’­ Orijinal Sentiment: {len(sentiment_data)} Ã¶rnek")
    print(f"   ğŸ“ˆ Orijinal Historical: {len(historical_df)} kayÄ±t")
    
    return qa_data, sentiment_data, historical_df

# STEP 3: Advanced Preprocessing
def preprocess_advanced_qa(qa_data):
    """GeliÅŸmiÅŸ Q&A preprocessing"""
    
    if not qa_data:
        print("âŒ Q&A verisi yok!")
        return [], None
    
    print(f"ğŸ”§ {len(qa_data)} Q&A Ã¶rneÄŸi geliÅŸmiÅŸ iÅŸleme...")
    
    # Turkish BERT
    model_name = "dbmdz/bert-base-turkish-cased"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    processed_examples = []
    
    for i, item in enumerate(qa_data):
        try:
            question = str(item.get("question", "")).strip()
            context = str(item.get("context", "")).strip()
            answer = str(item.get("answer", "")).strip()
            
            if not question or not context or not answer:
                continue
            
            # Tokenization
            encoding = tokenizer(
                question,
                context,
                max_length=512,
                truncation=True,
                padding="max_length",
                return_tensors="pt"
            )
            
            # Advanced answer position detection
            start_pos = 1  # [CLS] sonrasÄ±
            end_pos = start_pos + min(len(tokenizer.encode(answer, add_special_tokens=False)), 20)
            end_pos = min(end_pos, 510)
            
            processed_examples.append({
                "input_ids": encoding["input_ids"][0],
                "attention_mask": encoding["attention_mask"][0],
                "start_positions": torch.tensor(start_pos, dtype=torch.long),
                "end_positions": torch.tensor(end_pos, dtype=torch.long)
            })
            
            if (i + 1) % 50 == 0:
                print(f"   âš¡ Ä°ÅŸlendi: {i+1}/{len(qa_data)}")
                
        except Exception as e:
            continue
    
    print(f"âœ… {len(processed_examples)} Ã¶rnek baÅŸarÄ±yla iÅŸlendi!")
    return processed_examples, tokenizer

# STEP 4: Advanced Training
def train_advanced_model(processed_examples, tokenizer):
    """GeliÅŸmiÅŸ model eÄŸitimi"""
    
    if not processed_examples:
        print("âŒ Processed data yok!")
        return None, None
    
    print("ğŸ¤– ADVANCED MODEL TRAÄ°NÄ°NG...")
    
    # Model
    model_name = "dbmdz/bert-base-turkish-cased"
    model = AutoModelForQuestionAnswering.from_pretrained(model_name)
    
    print(f"ğŸ“¦ Model: {model.num_parameters():,} parametre")
    
    # Dataset
    train_dataset = Dataset.from_list(processed_examples)
    
    # Advanced TrainingArguments
    training_args = TrainingArguments(
        output_dir="./bist-ultimate-ai",
        learning_rate=2e-5,          # Optimal learning rate
        num_train_epochs=3,          # Daha fazla epoch
        per_device_train_batch_size=8,  # Daha bÃ¼yÃ¼k batch
        gradient_accumulation_steps=1,
        weight_decay=0.01,
        warmup_steps=20,
        save_steps=50,
        save_total_limit=2,
        logging_steps=10,
        push_to_hub=True,
        hub_model_id=HF_MODEL_NAME,
        fp16=True,
        dataloader_pin_memory=False,
        remove_unused_columns=False,
        report_to=None,
    )
    
    # Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        tokenizer=tokenizer,
        data_collator=DefaultDataCollator(),
    )
    
    print("ğŸ”¥ ADVANCED TRAÄ°NÄ°NG BAÅLIYOR...")
    start_time = datetime.now()
    
    try:
        train_result = trainer.train()
        
        duration = (datetime.now() - start_time).total_seconds()
        
        print(f"ğŸ‰ ADVANCED TRAÄ°NÄ°NG BAÅARILI!")
        print(f"ğŸ“Š Final Loss: {train_result.training_loss:.4f}")
        print(f"â° SÃ¼re: {duration:.1f} saniye")
        
        return trainer, model
        
    except Exception as e:
        print(f"âŒ Training error: {e}")
        # Smaller batch retry
        training_args.per_device_train_batch_size = 4
        training_args.gradient_accumulation_steps = 2
        
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            tokenizer=tokenizer,
            data_collator=DefaultDataCollator(),
        )
        
        try:
            train_result = trainer.train()
            print("âœ… KÃ¼Ã§Ã¼k batch ile baÅŸarÄ±lÄ±!")
            return trainer, model
        except Exception as e2:
            print(f"âŒ Ä°kinci deneme baÅŸarÄ±sÄ±z: {e2}")
            return None, None

# STEP 5: Comprehensive Test
def comprehensive_test(trainer, tokenizer):
    """KapsamlÄ± model testi"""
    
    if not trainer:
        return
    
    print("\nğŸ§ª COMPREHENSIVE MODEL TEST...")
    
    try:
        from transformers import pipeline
        
        qa_pipeline = pipeline(
            "question-answering",
            model=trainer.model,
            tokenizer=tokenizer,
            device=0 if torch.cuda.is_available() else -1
        )
        
        # Advanced test cases
        test_cases = [
            {
                "question": "BIST 100 endeksi nedir?",
                "context": "BIST 100 endeksi, Borsa Ä°stanbul'da iÅŸlem gÃ¶ren en bÃ¼yÃ¼k 100 ÅŸirketin piyasa deÄŸeri aÄŸÄ±rlÄ±klÄ± performansÄ±nÄ± yansÄ±tan ana gÃ¶stergedir. TÃ¼rkiye ekonomisinin nabzÄ±nÄ± tutan bu endeks gÃ¼nlÃ¼k olarak hesaplanÄ±r."
            },
            {
                "question": "RSI 70 Ã¼zerinde ne anlama gelir?",
                "context": "RSI (Relative Strength Index) 70 Ã¼zerindeki deÄŸerler hisse senedinin aÅŸÄ±rÄ± alÄ±m bÃ¶lgesinde olduÄŸunu gÃ¶sterir. Bu durumda satÄ±ÅŸ baskÄ±sÄ± artabilir ve fiyat dÃ¼zeltmesi beklenebilir."
            },
            {
                "question": "MACD sinyali nasÄ±l yorumlanÄ±r?",
                "context": "MACD gÃ¶stergesinde ana Ã§izginin sinyal Ã§izgisini yukarÄ± kesmesi gÃ¼Ã§lÃ¼ bir alÄ±m sinyali olarak kabul edilir. Bu durumda yÃ¼kseliÅŸ momentumu artmÄ±ÅŸ olur."
            },
            {
                "question": "Stop loss neden kullanÄ±lÄ±r?",
                "context": "Stop loss emri, yatÄ±rÄ±mcÄ±nÄ±n Ã¶nceden belirlediÄŸi zarar seviyesine ulaÅŸÄ±ldÄ±ÄŸÄ±nda otomatik olarak pozisyonu kapatÄ±r. Bu sayede kayÄ±plar sÄ±nÄ±rlanÄ±r ve sermaye korunur."
            },
            {
                "question": "Volatilite yÃ¼ksek olunca ne olur?",
                "context": "YÃ¼ksek volatilite dÃ¶nemlerinde hisse fiyatlarÄ± bÃ¼yÃ¼k dalgalanmalar gÃ¶sterir. Bu durum hem yÃ¼ksek kazanÃ§ fÄ±rsatlarÄ± hem de yÃ¼ksek risk anlamÄ±na gelir."
            }
        ]
        
        total_score = 0
        successful_tests = 0
        
        print("ğŸ“‹ ADVANCED TEST SONUÃ‡LARI:")
        print("=" * 70)
        
        for i, test in enumerate(test_cases, 1):
            try:
                result = qa_pipeline(
                    question=test["question"],
                    context=test["context"]
                )
                
                score = result['score']
                answer = result['answer']
                total_score += score
                successful_tests += 1
                
                print(f"Test {i}: {test['question']}")
                print(f"ğŸ¤– AI Cevap: {answer}")
                print(f"ğŸ¯ GÃ¼ven: {score:.3f} ({score*100:.1f}%)")
                
                # Quality assessment
                if score > 0.7:
                    quality = "ğŸŒŸ MÃ¼kemmel"
                elif score > 0.5:
                    quality = "â­ Ä°yi"
                elif score > 0.3:
                    quality = "ğŸ“ˆ Orta"
                else:
                    quality = "ğŸ”´ ZayÄ±f"
                
                print(f"ğŸ“Š Kalite: {quality}")
                print("=" * 70)
                
            except Exception as e:
                print(f"âŒ Test {i} error: {e}")
        
        if successful_tests > 0:
            avg_score = total_score / successful_tests
            print(f"\nğŸ“Š GENEL PERFORMANS RAPORU:")
            print(f"âœ… BaÅŸarÄ±lÄ± testler: {successful_tests}/{len(test_cases)}")
            print(f"ğŸ¯ Ortalama gÃ¼ven: {avg_score:.3f} ({avg_score*100:.1f}%)")
            
            if avg_score > 0.7:
                overall = "ğŸŒŸ MÃœKEMMEL - Production Ready!"
            elif avg_score > 0.5:
                overall = "â­ Ä°YÄ° - KullanÄ±labilir"
            elif avg_score > 0.3:
                overall = "ğŸ“ˆ ORTA - Ä°yileÅŸtirilebilir"
            else:
                overall = "ğŸ”´ ZAYIF - Daha fazla eÄŸitim gerekli"
            
            print(f"ğŸ† Genel DeÄŸerlendirme: {overall}")
            
    except Exception as e:
        print(f"âŒ Test genel hatasÄ±: {e}")

# MAIN EXECUTION
def main():
    """Ana ultimate training fonksiyonu"""
    
    print("ğŸ¯ ULTIMATE TRAINING PIPELINE")
    print("=" * 60)
    
    # Full data loading
    qa_data, sentiment_data, historical_df = load_full_training_data()
    
    if len(qa_data) < 10:
        print("âŒ Yetersiz Q&A verisi!")
        return {'status': 'failed', 'reason': 'insufficient_data'}
    
    # Advanced preprocessing
    processed_examples, tokenizer = preprocess_advanced_qa(qa_data)
    
    if not processed_examples:
        print("âŒ Preprocessing baÅŸarÄ±sÄ±z!")
        return {'status': 'failed', 'reason': 'preprocessing_failed'}
    
    # Advanced training
    trainer, model = train_advanced_model(processed_examples, tokenizer)
    
    # Comprehensive testing
    comprehensive_test(trainer, tokenizer)
    
    # Deploy
    deploy_success = False
    if trainer:
        try:
            trainer.push_to_hub(
                commit_message="Ultimate Turkish Financial AI - Full Dataset Training"
            )
            print("\nğŸš€ ULTIMATE DEPLOY BAÅARILI!")
            print(f"ğŸ“ Model: https://huggingface.co/{HF_MODEL_NAME}")
            print(f"ğŸ”— API: https://api-inference.huggingface.co/models/{HF_MODEL_NAME}")
            deploy_success = True
        except Exception as e:
            print(f"âŒ Deploy error: {e}")
    
    # Final summary
    print("\n" + "=" * 60)
    print("ğŸ‰ ULTIMATE TRAINING TAMAMLANDI!")
    print("=" * 60)
    print(f"âœ… Toplam Q&A: {len(qa_data)} Ã¶rnek")
    print(f"âœ… Ä°ÅŸlenen: {len(processed_examples)} Ã¶rnek")
    print(f"âœ… Training: {'BaÅŸarÄ±lÄ±' if trainer else 'BaÅŸarÄ±sÄ±z'}")
    print(f"âœ… Deploy: {'BaÅŸarÄ±lÄ±' if deploy_success else 'BaÅŸarÄ±sÄ±z'}")
    print("=" * 60)
    
    return {
        'status': 'success' if trainer else 'failed',
        'total_qa': len(qa_data),
        'processed': len(processed_examples),
        'sentiment_used': len(sentiment_data),
        'historical_used': len(historical_df),
        'deploy_success': deploy_success,
        'model_name': HF_MODEL_NAME
    }

# EXECUTION
if __name__ == "__main__":
    print("ğŸ”¥ ULTIMATE TRAINING Ã‡ALIÅTIRILIYOR...")
    print("ğŸ’¡ GPU:", "âœ… Aktif" if torch.cuda.is_available() else "âŒ CPU")
    
    if torch.cuda.is_available():
        print(f"   ğŸ® GPU: {torch.cuda.get_device_name(0)}")
    
    print("\n" + "ğŸš€ " * 20)
    result = main()
    print("ğŸš€ " * 20)
    
    print(f"\nğŸŠ ULTIMATE RESULT:")
    print(f"Status: {result['status']}")
    if result['status'] == 'success':
        print(f"Model: {result['model_name']}")
        print(f"Total Q&A: {result['total_qa']}")
        print(f"Sentiment Used: {result['sentiment_used']}")
        print(f"Historical Used: {result['historical_used']}")
        print(f"Deploy: {'ğŸ‰ SUCCESS' if result['deploy_success'] else 'ğŸ’¾ Local'}")
        
        print(f"\nğŸ¯ RAILWAY INTEGRATION:")
        print(f"API: https://api-inference.huggingface.co/models/{result['model_name']}")
        print(f"Token: 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")
    
    print("\nğŸ”¥ ULTIMATE FIX COMPLETE! ğŸ”¥")
