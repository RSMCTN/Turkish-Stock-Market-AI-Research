#!/usr/bin/env python3
"""
ğŸš€ COLAB FIX - DOSYA YOLU DÃœZELTÄ°LDÄ°!
Bu version Colab'daki root dizinde dosyalarÄ± arar
"""

import json
import pandas as pd
import numpy as np
import torch
from datetime import datetime
import warnings
import os
warnings.filterwarnings('ignore')

print("ğŸš€ COLAB FIXED - ADVANCED TURKISH FINANCIAL AI TRAINING")
print("ğŸ¯ 117 Sembol + 30 Ä°ndikatÃ¶r + 1.4M Historical Data")
print("=" * 60)

# DOSYA YOLLARINI KONTROL ET
def check_files():
    """Colab'daki dosyalarÄ± kontrol et"""
    print("ğŸ“ Dosya kontrolÃ¼ yapÄ±lÄ±yor...")
    
    # Mevcut dizindeki dosyalarÄ± listele
    files = os.listdir('.')
    print(f"ğŸ“‚ Mevcut dizin: {os.getcwd()}")
    print(f"ğŸ“‹ Dosyalar: {[f for f in files if f.endswith(('.json', '.csv'))]}")
    
    # Training dosyalarÄ±nÄ± ara
    required_files = {
        'qa': ['enhanced_turkish_qa.json', 'training_data/enhanced_turkish_qa.json'],
        'sentiment': ['enhanced_sentiment.json', 'training_data/enhanced_sentiment.json'],
        'historical': ['enhanced_historical_training.csv', 'training_data/enhanced_historical_training.csv']
    }
    
    found_files = {}
    
    for file_type, possible_paths in required_files.items():
        found = False
        for path in possible_paths:
            if os.path.exists(path):
                found_files[file_type] = path
                print(f"âœ… {file_type.upper()}: {path}")
                found = True
                break
        
        if not found:
            print(f"âŒ {file_type.upper()}: BulunamadÄ±!")
            found_files[file_type] = None
    
    return found_files

# STEP 1: HuggingFace Setup
try:
    from huggingface_hub import login
    from transformers import AutoTokenizer, AutoModelForQuestionAnswering
    from transformers import TrainingArguments, Trainer, DefaultDataCollator
    from datasets import Dataset
    
    print("âœ… Dependencies loaded successfully!")
except ImportError as e:
    print(f"âŒ Dependency error: {e}")
    print("ğŸ’¡ Colab'da ÅŸunu Ã§alÄ±ÅŸtÄ±r:")
    print("!pip install transformers datasets torch huggingface_hub accelerate")
    exit(1)

# HF Authentication
HF_TOKEN = "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"
HF_MODEL_NAME = "rsmctn/bist-advanced-turkish-ai-v2"

try:
    login(HF_TOKEN)
    print("âœ… HuggingFace authenticated!")
except Exception as e:
    print(f"âŒ HF Auth error: {e}")

# STEP 2: Load Training Data - FIXED PATHS
def load_training_data_fixed():
    """COLAB iÃ§in dÃ¼zeltilmiÅŸ dosya yollarÄ±"""
    
    print("ğŸ“Š Enhanced training data yÃ¼kleniyor (FIXED PATHS)...")
    
    # Dosya yerlerini kontrol et
    file_paths = check_files()
    
    # Q&A Dataset
    if file_paths['qa']:
        try:
            with open(file_paths['qa'], 'r', encoding='utf-8') as f:
                qa_data = json.load(f)
            print(f"âœ… Q&A Data: {len(qa_data)} soru-cevap Ã§ifti")
        except Exception as e:
            print(f"âŒ Q&A yÃ¼kleme hatasÄ±: {e}")
            qa_data = []
    else:
        print("âŒ Q&A dosyasÄ± bulunamadÄ±!")
        qa_data = []
    
    # Sentiment Dataset  
    if file_paths['sentiment']:
        try:
            with open(file_paths['sentiment'], 'r', encoding='utf-8') as f:
                sentiment_data = json.load(f)
            print(f"âœ… Sentiment Data: {len(sentiment_data)} sentiment Ã¶rneÄŸi")
        except Exception as e:
            print(f"âŒ Sentiment yÃ¼kleme hatasÄ±: {e}")
            sentiment_data = []
    else:
        print("âš ï¸ Sentiment data bulunamadÄ±, atlanÄ±yor...")
        sentiment_data = []
    
    # Historical Dataset
    if file_paths['historical']:
        try:
            historical_df = pd.read_csv(file_paths['historical'])
            print(f"âœ… Historical Data: {len(historical_df)} veri noktasÄ±")
            if 'symbol' in historical_df.columns:
                print(f"ğŸ“Š Semboller: {historical_df['symbol'].nunique()} benzersiz sembol")
            if 'date' in historical_df.columns:
                print(f"ğŸ“ˆ Tarih aralÄ±ÄŸÄ±: {historical_df['date'].min()} â†’ {historical_df['date'].max()}")
        except Exception as e:
            print(f"âŒ Historical yÃ¼kleme hatasÄ±: {e}")
            historical_df = pd.DataFrame()
    else:
        print("âš ï¸ Historical CSV bulunamadÄ±, atlanÄ±yor...")
        historical_df = pd.DataFrame()
    
    return qa_data, sentiment_data, historical_df

# FALLBACK DATA - EÄŸer dosyalar yoksa
def create_fallback_data():
    """Dosya yoksa fallback dataset oluÅŸtur"""
    print("ğŸ”„ FALLBACK: Minimum dataset oluÅŸturuluyor...")
    
    fallback_qa = [
        {
            "question": "BIST 100 endeksi nedir?",
            "context": "BIST 100 endeksi, Borsa Ä°stanbul'da iÅŸlem gÃ¶ren en bÃ¼yÃ¼k 100 ÅŸirketin performansÄ±nÄ± gÃ¶steren ana endekstir. PiyasanÄ±n genel yÃ¶nÃ¼nÃ¼ yansÄ±tÄ±r.",
            "answer": "BIST 100, en bÃ¼yÃ¼k 100 ÅŸirketin performansÄ±nÄ± gÃ¶steren ana endekstir"
        },
        {
            "question": "RSI nedir?",
            "context": "RSI (Relative Strength Index) 0-100 arasÄ±nda deÄŸer alan momentum osilatÃ¶rÃ¼dÃ¼r. 70 Ã¼zerinde aÅŸÄ±rÄ± alÄ±m, 30 altÄ±nda aÅŸÄ±rÄ± satÄ±m bÃ¶lgesini gÃ¶sterir.",
            "answer": "RSI, 0-100 arasÄ±nda deÄŸer alan momentum gÃ¶stergesidir"
        },
        {
            "question": "Teknik analiz nasÄ±l yapÄ±lÄ±r?",
            "context": "Teknik analiz, geÃ§miÅŸ fiyat hareketleri ve iÅŸlem hacmi verilerini kullanarak gelecekteki fiyat hareketlerini tahmin etme yÃ¶ntemidir.",
            "answer": "Teknik analiz, geÃ§miÅŸ fiyat verilerini kullanarak gelecek tahminleri yapar"
        }
    ]
    
    print(f"âœ… Fallback Q&A: {len(fallback_qa)} Ã¶rnek")
    return fallback_qa, [], pd.DataFrame()

# STEP 3: Enhanced Data Preprocessing - SAME AS BEFORE
def preprocess_enhanced_qa_data(qa_data):
    """Enhanced Q&A data'yÄ± model iÃ§in hazÄ±rla"""
    
    if not qa_data:
        print("âŒ Q&A verisi yok!")
        return [], None
    
    print(f"ğŸ”§ {len(qa_data)} Q&A Ã¶rneÄŸi iÅŸleniyor...")
    
    processed_examples = []
    
    # Turkish BERT tokenizer
    model_name = "dbmdz/bert-base-turkish-cased"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    for i, item in enumerate(qa_data):
        try:
            question = item["question"]
            context = item["context"] 
            answer = item["answer"]
            
            # Tokenize
            encoding = tokenizer(
                question,
                context,
                max_length=512,
                truncation=True,
                padding="max_length",
                return_tensors="pt"
            )
            
            # Answer positions
            answer_start = context.find(answer)
            if answer_start >= 0:
                start_pos = 1  # After [CLS]
                end_pos = min(start_pos + len(tokenizer.encode(answer, add_special_tokens=False)), 510)
            else:
                start_pos = 1
                end_pos = 2
            
            processed_examples.append({
                "input_ids": encoding["input_ids"][0],
                "attention_mask": encoding["attention_mask"][0],
                "start_positions": torch.tensor(start_pos, dtype=torch.long),
                "end_positions": torch.tensor(end_pos, dtype=torch.long)
            })
            
        except Exception as e:
            print(f"âš ï¸ Ã–rnek {i} atlandÄ±: {e}")
            continue
    
    print(f"âœ… {len(processed_examples)} Ã¶rnek baÅŸarÄ±yla iÅŸlendi!")
    return processed_examples, tokenizer

# STEP 4: Quick Training - Colab iÃ§in optimize
def train_quick_model(processed_examples, tokenizer):
    """HÄ±zlÄ± model eÄŸitimi - Colab iÃ§in optimize"""
    
    if not processed_examples:
        print("âŒ Ä°ÅŸlenmiÅŸ Ã¶rnek yok!")
        return None, None
        
    print("ğŸ¤– HÄ±zlÄ± model eÄŸitimi baÅŸlÄ±yor...")
    
    # Turkish BERT model
    model_name = "dbmdz/bert-base-turkish-cased"
    model = AutoModelForQuestionAnswering.from_pretrained(model_name)
    
    print(f"ğŸ“¦ Model yÃ¼klendi: {model.num_parameters():,} parametre")
    
    # Dataset oluÅŸtur
    train_dataset = Dataset.from_list(processed_examples)
    
    # Quick training arguments
    training_args = TrainingArguments(
        output_dir="./bist-quick-turkish-ai",
        learning_rate=5e-5,  # HÄ±zlÄ± Ã¶ÄŸrenme
        num_train_epochs=2,  # KÄ±sa eÄŸitim
        per_device_train_batch_size=8,
        per_device_eval_batch_size=8,
        gradient_accumulation_steps=1,
        weight_decay=0.01,
        warmup_steps=10,
        evaluation_strategy="no",  # Evaluation kapalÄ±
        save_steps=50,
        save_total_limit=1,
        logging_steps=5,
        push_to_hub=True,
        hub_model_id=HF_MODEL_NAME,
        hub_strategy="end",
        fp16=True,
        dataloader_pin_memory=False,
        remove_unused_columns=False,
        report_to=None,
    )
    
    # Trainer oluÅŸtur
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        tokenizer=tokenizer,
        data_collator=DefaultDataCollator(),
    )
    
    print("ğŸ”¥ HIZLI EÄÄ°TÄ°M BAÅLATIYOR...")
    print(f"â° BaÅŸlama zamanÄ±: {datetime.now().strftime('%H:%M:%S')}")
    
    try:
        train_result = trainer.train()
        print(f"ğŸ‰ EÄÄ°TÄ°M TAMAMLANDI! Loss: {train_result.training_loss:.4f}")
        return trainer, model
        
    except Exception as e:
        print(f"âŒ Training error: {e}")
        return None, None

# STEP 5: Quick Test
def test_quick_model(trainer, tokenizer):
    """HÄ±zlÄ± model test"""
    
    if not trainer:
        print("âŒ Model test edilemedi!")
        return
    
    print("\nğŸ§ª HIZLI TEST...")
    
    from transformers import pipeline
    
    qa_pipeline = pipeline(
        "question-answering",
        model=trainer.model,
        tokenizer=tokenizer,
        device=0 if torch.cuda.is_available() else -1
    )
    
    test_cases = [
        {
            "question": "BIST 100 nedir?",
            "context": "BIST 100 endeksi TÃ¼rkiye'nin en bÃ¼yÃ¼k 100 ÅŸirketinin performansÄ±nÄ± gÃ¶steren ana borsa endeksidir."
        },
        {
            "question": "RSI nasÄ±l kullanÄ±lÄ±r?",
            "context": "RSI 70 Ã¼zerinde aÅŸÄ±rÄ± alÄ±m, 30 altÄ±nda aÅŸÄ±rÄ± satÄ±m sinyali verir. Bu teknik analiz gÃ¶stergesi momentum Ã¶lÃ§er."
        }
    ]
    
    for i, test in enumerate(test_cases, 1):
        try:
            result = qa_pipeline(question=test["question"], context=test["context"])
            print(f"Test {i}: {test['question']}")
            print(f"ğŸ¤– Cevap: {result['answer']}")
            print(f"ğŸ¯ GÃ¼ven: {result['score']:.3f}")
            print("-" * 40)
        except Exception as e:
            print(f"âŒ Test {i} hatasÄ±: {e}")

# MAIN EXECUTION - FIXED
def main_fixed():
    """DÃ¼zeltilmiÅŸ ana fonksiyon"""
    
    print("ğŸ¯ COLAB FIXED TRAINING PIPELINE")
    print("=" * 60)
    
    # Load data with fixed paths
    qa_data, sentiment_data, historical_df = load_training_data_fixed()
    
    # Fallback if no data
    if not qa_data:
        print("ğŸ”„ Fallback data kullanÄ±lÄ±yor...")
        qa_data, sentiment_data, historical_df = create_fallback_data()
    
    if not qa_data:
        print("âŒ HiÃ§ veri yok! Training durduruluyor.")
        return {
            'model_name': HF_MODEL_NAME,
            'qa_samples': 0,
            'processed_samples': 0,
            'deploy_success': False
        }
    
    print(f"\nğŸ“Š DATASET SUMMARY:")
    print(f"   ğŸ¯ Q&A Samples: {len(qa_data)}")
    print(f"   ğŸ’­ Sentiment Samples: {len(sentiment_data)}")
    print(f"   ğŸ“ˆ Historical Points: {len(historical_df) if not historical_df.empty else 0}")
    
    # Preprocess
    processed_examples, tokenizer = preprocess_enhanced_qa_data(qa_data)
    
    if not processed_examples:
        print("âŒ Veri iÅŸlenemedi!")
        return {
            'model_name': HF_MODEL_NAME,
            'qa_samples': len(qa_data),
            'processed_samples': 0,
            'deploy_success': False
        }
    
    # Train
    trainer, model = train_quick_model(processed_examples, tokenizer)
    
    # Test
    if trainer:
        test_quick_model(trainer, tokenizer)
    
    # Deploy
    deploy_success = False
    if trainer:
        try:
            trainer.push_to_hub(commit_message="Quick Turkish Financial AI - Colab Fixed")
            print("ğŸ‰ DEPLOY BAÅARILI!")
            deploy_success = True
        except Exception as e:
            print(f"âš ï¸ Deploy hatasÄ±: {e}")
    
    print("\n" + "=" * 60)
    print("ğŸ‰ COLAB FIX TRAINING TAMAMLANDI!")
    print("=" * 60)
    
    return {
        'model_name': HF_MODEL_NAME,
        'qa_samples': len(qa_data),
        'processed_samples': len(processed_examples),
        'deploy_success': deploy_success
    }

# COLAB EXECUTION
if __name__ == "__main__":
    print("ğŸ”¥ COLAB FIXED VERSION Ã‡ALIÅTIRILIYOR...")
    print("ğŸ’¡ GPU kullanÄ±mÄ±:", "âœ… Aktif" if torch.cuda.is_available() else "âŒ CPU")
    
    if torch.cuda.is_available():
        print(f"   ğŸ® GPU: {torch.cuda.get_device_name(0)}")
        print(f"   ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    
    print("\n" + "ğŸš€ " * 20)
    result = main_fixed()
    print("ğŸš€ " * 20)
    
    print(f"\nğŸŠ FINAL RESULT:")
    print(f"Model: {result['model_name']}")
    print(f"Samples: {result['qa_samples']} â†’ {result['processed_samples']}")
    print(f"Status: {'ğŸ‰ Production Ready!' if result['deploy_success'] else 'ğŸ’¾ Local Backup'}")
